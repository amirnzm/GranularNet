{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd6fa25",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Network (PINN) for Granular Segregation - Phase 1\n",
    "\n",
    "This notebook implements a PINN to solve the non-dimensionalized segregation PDE.\n",
    "\n",
    "## Non-dimensionalized PDE\n",
    "\n",
    "The governing equation is:\n",
    "\n",
    "$$\\frac{\\partial c}{\\partial \\tilde{t}} + \\tilde{u} \\frac{\\partial c}{\\partial \\tilde{x}} + \\tilde{w} \\frac{\\partial c}{\\partial \\tilde{z}} + \\Lambda(1 - \\tilde{x}) \\frac{\\partial}{\\partial \\tilde{z}} \\left[ g(\\tilde{z}) c(1-c) \\right] = \\frac{\\partial}{\\partial \\tilde{z}} \\left[ \\frac{1}{Pe} \\frac{\\partial c}{\\partial \\tilde{z}} \\right]$$\n",
    "\n",
    "Where:\n",
    "- $c$ is the concentration (volume fraction of small particles)\n",
    "- $\\tilde{x} \\in [0, 1]$, $\\tilde{z} \\in [-1, 0]$, $\\tilde{t} \\in [0, t_{end}]$ are dimensionless coordinates\n",
    "- $\\Lambda$ is the segregation parameter\n",
    "- $Pe$ is the Péclet number\n",
    "- $\\tilde{u}$ and $\\tilde{w}$ are dimensionless velocity profiles\n",
    "- $g(\\tilde{z})$ is a shear-rate-like profile\n",
    "\n",
    "## Boundary Conditions\n",
    "\n",
    "- **Inlet** ($\\tilde{x} = 0$): Dirichlet $c = 0.5$ (well-mixed feed)\n",
    "- **Top/Bottom** ($\\tilde{z} = 0, -1$): Neumann $(1/Pe) \\partial c/\\partial \\tilde{z} = \\Lambda(1-\\tilde{x}) g(\\tilde{z}) c(1-c)$\n",
    "- **Outlet** ($\\tilde{x} = 1$): Natural boundary condition (zero diffusive flux)\n",
    "- **Initial condition**: $c = 0.5$ everywhere at $\\tilde{t} = 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa83b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import grad\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe57e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters from the paper (section 3)\n",
    "Lambda = 0.3949      # segregation parameter Λ\n",
    "Pe = 27.5387        # Péclet number\n",
    "D = 1.0 / Pe      # vertical diffusivity\n",
    "k = 2.3           # exponential velocity profile parameter\n",
    "tEnd = 20.0        # final dimensionless time\n",
    "\n",
    "# Domain bounds\n",
    "x_min, x_max = 0.0, 1.0    # x̃ ∈ [0, 1]\n",
    "z_min, z_max = -1.0, 0.0   # z̃ ∈ [-1, 0]\n",
    "t_min, t_max = 0.0, tEnd   # t̃ ∈ [0, tEnd]\n",
    "\n",
    "print(f\"Parameters: Λ={Lambda}, Pe={Pe}, D={D:.6f}, k={k}\")\n",
    "print(f\"Domain: x̃ ∈ [{x_min}, {x_max}], z̃ ∈ [{z_min}, {z_max}], t̃ ∈ [{t_min}, {t_max}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5032ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for velocity profiles and g(z̃)\n",
    "def u_tilde(x, z, k):\n",
    "    \"\"\"Dimensionless velocity in x direction\"\"\"\n",
    "    # Convert k to tensor if it's not already a tensor\n",
    "    if not isinstance(k, torch.Tensor):\n",
    "        k = torch.tensor(float(k), dtype=x.dtype, device=x.device)\n",
    "    factor = 0.5 * k * (1 - torch.exp(-k))\n",
    "    return factor * (1 - x) * torch.exp(k * z)\n",
    "\n",
    "def w_tilde(z, k):\n",
    "    \"\"\"Dimensionless velocity in z direction\"\"\"\n",
    "    # Convert k to tensor if it's not already a tensor\n",
    "    if not isinstance(k, torch.Tensor):\n",
    "        k = torch.tensor(float(k), dtype=z.dtype, device=z.device)\n",
    "    factor = 0.5 * (1 - torch.exp(-k))\n",
    "    return factor * (torch.exp(k * z) - 1)\n",
    "\n",
    "def g_profile(z, k):\n",
    "    \"\"\"Shear-rate-like profile g(z̃) and its derivative\"\"\"\n",
    "    # Convert k to tensor if it's not already a tensor\n",
    "    if not isinstance(k, torch.Tensor):\n",
    "        k = torch.tensor(float(k), dtype=z.dtype, device=z.device)\n",
    "    g_val = (k**2 / (2 * (1 - torch.exp(-k)))) * torch.exp(k * z)\n",
    "    g_prime = k * g_val\n",
    "    return g_val, g_prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bfad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Architecture\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(PINN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        for layer in self.layers:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, x, z, t):\n",
    "        # Concatenate inputs: [x, z, t]\n",
    "        inputs = torch.cat([x, z, t], dim=1)\n",
    "        \n",
    "        # Forward pass through hidden layers\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            inputs = torch.tanh(layer(inputs))\n",
    "        \n",
    "        # Output layer (no activation for regression)\n",
    "        # output = self.layers[-1](inputs)\n",
    "        \n",
    "        # Output layer with sigmoid activation to bound concentration between 0 and 1\n",
    "        output = torch.sigmoid(self.layers[-1](inputs))\n",
    "        return output\n",
    "\n",
    "# Network architecture: 3 inputs (x, z, t) -> hidden layers -> 1 output (c)\n",
    "layers = [3, 64, 64, 64, 64, 1]\n",
    "model = PINN(layers).to(device)\n",
    "print(f\"Model architecture: {layers}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9a8978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pde_residual(x, z, t, model, Lambda, Pe, k):\n",
    "    \"\"\"\n",
    "    Compute the PDE residual:\n",
    "    ∂c/∂t̃ = (1/Pe) ∂²c/∂z̃² - ũ ∂c/∂x̃ - w̃ ∂c/∂z̃ \n",
    "            - Λ(1-x̃)[ g'(z̃)c(1-c) + g(z̃)(1-2c)∂c/∂z̃ ]\n",
    "    \"\"\"\n",
    "    # Clone and detach tensors to avoid modifying originals, then enable gradient computation\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    z = z.clone().detach().requires_grad_(True)\n",
    "    t = t.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Forward pass\n",
    "    c = model(x, z, t)\n",
    "    \n",
    "    # Compute gradients\n",
    "    c_t = grad(c, t, grad_outputs=torch.ones_like(c), create_graph=True)[0]\n",
    "    c_x = grad(c, x, grad_outputs=torch.ones_like(c), create_graph=True)[0]\n",
    "    c_z = grad(c, z, grad_outputs=torch.ones_like(c), create_graph=True)[0]\n",
    "    \n",
    "    # Second derivative in z\n",
    "    c_zz = grad(c_z, z, grad_outputs=torch.ones_like(c_z), create_graph=True)[0]\n",
    "    \n",
    "    # Velocity profiles\n",
    "    u_tilde_val = u_tilde(x, z, k)\n",
    "    w_tilde_val = w_tilde(z, k)\n",
    "    \n",
    "    # g profile and its derivative\n",
    "    g_val, g_prime = g_profile(z, k)\n",
    "    \n",
    "    # Diffusion term\n",
    "    diff_term = (1.0 / Pe) * c_zz\n",
    "    \n",
    "    # Advection term\n",
    "    adv_term = u_tilde_val * c_x + w_tilde_val * c_z\n",
    "    \n",
    "    # Segregation term\n",
    "    seg_term = Lambda * (1 - x) * (g_prime * c * (1 - c) + g_val * (1 - 2*c) * c_z)\n",
    "    \n",
    "    # PDE residual: ∂c/∂t - (diffusion - advection - segregation)\n",
    "    residual = c_t - (diff_term - adv_term - seg_term)\n",
    "    \n",
    "    return residual, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd61db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training points\n",
    "def generate_training_data(n_pde, n_bc_inlet, n_bc_top, n_bc_bottom, n_bc_outlet, n_ic):\n",
    "    \"\"\"\n",
    "    Generate collocation points for:\n",
    "    - PDE residual points (interior)\n",
    "    - Boundary condition points\n",
    "    - Initial condition points\n",
    "    \"\"\"\n",
    "    # PDE collocation points (interior domain)\n",
    "    x_pde = torch.rand(n_pde, 1, device=device) * (x_max - x_min) + x_min\n",
    "    z_pde = torch.rand(n_pde, 1, device=device) * (z_max - z_min) + z_min\n",
    "    t_pde = torch.rand(n_pde, 1, device=device) * (t_max - t_min) + t_min\n",
    "    \n",
    "    # Boundary: x̃ = 0 (inlet, Dirichlet: c = 0.5)\n",
    "    x_bc_inlet = torch.zeros(n_bc_inlet, 1, device=device)\n",
    "    z_bc_inlet = torch.rand(n_bc_inlet, 1, device=device) * (z_max - z_min) + z_min\n",
    "    t_bc_inlet = torch.rand(n_bc_inlet, 1, device=device) * (t_max - t_min) + t_min\n",
    "    \n",
    "    # Boundary: z̃ = 0 (top, Neumann)\n",
    "    x_bc_top = torch.rand(n_bc_top, 1, device=device) * (x_max - x_min) + x_min\n",
    "    z_bc_top = torch.zeros(n_bc_top, 1, device=device)\n",
    "    t_bc_top = torch.rand(n_bc_top, 1, device=device) * (t_max - t_min) + t_min\n",
    "    \n",
    "    # Boundary: z̃ = -1 (bottom, Neumann)\n",
    "    x_bc_bottom = torch.rand(n_bc_bottom, 1, device=device) * (x_max - x_min) + x_min\n",
    "    z_bc_bottom = torch.ones(n_bc_bottom, 1, device=device) * z_min\n",
    "    t_bc_bottom = torch.rand(n_bc_bottom, 1, device=device) * (t_max - t_min) + t_min\n",
    "    \n",
    "    # Boundary: x̃ = 1 (outlet, zero diffusive flux - natural BC, handled by PDE)\n",
    "    x_bc_outlet = torch.ones(n_bc_outlet, 1, device=device)\n",
    "    z_bc_outlet = torch.rand(n_bc_outlet, 1, device=device) * (z_max - z_min) + z_min\n",
    "    t_bc_outlet = torch.rand(n_bc_outlet, 1, device=device) * (t_max - t_min) + t_min\n",
    "    \n",
    "    # Initial condition: t̃ = 0, c = 0.5 everywhere\n",
    "    x_ic = torch.rand(n_ic, 1, device=device) * (x_max - x_min) + x_min\n",
    "    z_ic = torch.rand(n_ic, 1, device=device) * (z_max - z_min) + z_min\n",
    "    t_ic = torch.zeros(n_ic, 1, device=device)\n",
    "    \n",
    "    return {\n",
    "        'pde': (x_pde, z_pde, t_pde),\n",
    "        'bc_inlet': (x_bc_inlet, z_bc_inlet, t_bc_inlet),\n",
    "        'bc_top': (x_bc_top, z_bc_top, t_bc_top),\n",
    "        'bc_bottom': (x_bc_bottom, z_bc_bottom, t_bc_bottom),\n",
    "        'bc_outlet': (x_bc_outlet, z_bc_outlet, t_bc_outlet),\n",
    "        'ic': (x_ic, z_ic, t_ic)\n",
    "    }\n",
    "\n",
    "# Number of training points\n",
    "n_pde = 10000\n",
    "n_bc_inlet = 1000\n",
    "n_bc_top = 1000\n",
    "n_bc_bottom = 1000\n",
    "n_bc_outlet = 1000\n",
    "n_ic = 2000\n",
    "\n",
    "train_data = generate_training_data(n_pde, n_bc_inlet, n_bc_top, n_bc_bottom, n_bc_outlet, n_ic)\n",
    "print(\"Training data generated:\")\n",
    "for key, (x, z, t) in train_data.items():\n",
    "    print(f\"  {key}: {x.shape[0]} points\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad98d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, train_data, Lambda, Pe, k, weights):\n",
    "    \"\"\"\n",
    "    Compute total loss:\n",
    "    L = w_pde * L_pde + w_bc_inlet * L_bc_inlet + w_bc_top * L_bc_top \n",
    "        + w_bc_bottom * L_bc_bottom + w_ic * L_ic\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # PDE residual loss\n",
    "    x_pde, z_pde, t_pde = train_data['pde']\n",
    "    residual, _ = compute_pde_residual(x_pde, z_pde, t_pde, model, Lambda, Pe, k)\n",
    "    loss_pde = torch.mean(residual**2)\n",
    "    total_loss += weights['pde'] * loss_pde\n",
    "    \n",
    "    # Boundary condition: x̃ = 0, c = 0.5 (Dirichlet)\n",
    "    x_in, z_in, t_in = train_data['bc_inlet']\n",
    "    c_in = model(x_in, z_in, t_in)\n",
    "    loss_bc_inlet = torch.mean((c_in - 0.5)**2)\n",
    "    total_loss += weights['bc_inlet'] * loss_bc_inlet\n",
    "    \n",
    "    # Boundary condition: z̃ = 0 (top, Neumann: (1/Pe) ∂c/∂z = Λ(1-x) g(z) c(1-c))\n",
    "    # Clone tensors to avoid modifying originals\n",
    "    x_top, z_top, t_top = train_data['bc_top']\n",
    "    x_top = x_top.clone().detach().requires_grad_(True)\n",
    "    z_top = z_top.clone().detach().requires_grad_(True)\n",
    "    t_top = t_top.clone().detach()\n",
    "    c_top = model(x_top, z_top, t_top)\n",
    "    c_z_top = grad(c_top, z_top, grad_outputs=torch.ones_like(c_top), create_graph=True)[0]\n",
    "    g_val_top, _ = g_profile(z_top, k)\n",
    "    bc_top_target = Lambda * (1 - x_top) * g_val_top * c_top * (1 - c_top)\n",
    "    bc_top_pred = (1.0 / Pe) * c_z_top\n",
    "    loss_bc_top = torch.mean((bc_top_pred - bc_top_target)**2)\n",
    "    total_loss += weights['bc_top'] * loss_bc_top\n",
    "    \n",
    "    # Boundary condition: z̃ = -1 (bottom, Neumann: (1/Pe) ∂c/∂z = Λ(1-x) g(z) c(1-c))\n",
    "    # Clone tensors to avoid modifying originals\n",
    "    x_bot, z_bot, t_bot = train_data['bc_bottom']\n",
    "    x_bot = x_bot.clone().detach().requires_grad_(True)\n",
    "    z_bot = z_bot.clone().detach().requires_grad_(True)\n",
    "    t_bot = t_bot.clone().detach()\n",
    "    c_bot = model(x_bot, z_bot, t_bot)\n",
    "    c_z_bot = grad(c_bot, z_bot, grad_outputs=torch.ones_like(c_bot), create_graph=True)[0]\n",
    "    g_val_bot, _ = g_profile(z_bot, k)\n",
    "    bc_bot_target = Lambda * (1 - x_bot) * g_val_bot * c_bot * (1 - c_bot)\n",
    "    bc_bot_pred = (1.0 / Pe) * c_z_bot\n",
    "    loss_bc_bottom = torch.mean((bc_bot_pred - bc_bot_target)**2)\n",
    "    total_loss += weights['bc_bottom'] * loss_bc_bottom\n",
    "    \n",
    "    # Initial condition: t̃ = 0, c = 0.5\n",
    "    x_ic, z_ic, t_ic = train_data['ic']\n",
    "    c_ic = model(x_ic, z_ic, t_ic)\n",
    "    loss_ic = torch.mean((c_ic - 0.5)**2)\n",
    "    total_loss += weights['ic'] * loss_ic\n",
    "    \n",
    "    # Outlet boundary (x̃ = 1): zero diffusive flux is naturally satisfied\n",
    "    # by the PDE, so we don't need to enforce it explicitly\n",
    "    \n",
    "    return total_loss, {\n",
    "        'pde': loss_pde.item(),\n",
    "        'bc_inlet': loss_bc_inlet.item(),\n",
    "        'bc_top': loss_bc_top.item(),\n",
    "        'bc_bottom': loss_bc_bottom.item(),\n",
    "        'ic': loss_ic.item()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28971d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1000)\n",
    "\n",
    "# Loss weights (can be adjusted)\n",
    "loss_weights = {\n",
    "    'pde': 1.0,\n",
    "    'bc_inlet': 10.0,\n",
    "    'bc_top': 10.0,\n",
    "    'bc_bottom': 10.0,\n",
    "    'ic': 10.0\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 20000\n",
    "print_interval = 500\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs) # option 2: cosine annealing\n",
    "\n",
    "# Training loop\n",
    "loss_history = []\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"{'Epoch':<10} {'Total Loss':<15} {'PDE':<15} {'BC Inlet':<15} {'BC Top':<15} {'BC Bottom':<15} {'IC':<15}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "try:\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute loss\n",
    "        total_loss, loss_dict = compute_loss(model, train_data, Lambda, Pe, k, loss_weights)\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "#         scheduler.step(total_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Store loss history\n",
    "        loss_history.append({\n",
    "            'epoch': epoch,\n",
    "            'total': total_loss.item(),\n",
    "            **loss_dict\n",
    "        })\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % print_interval == 0 or epoch == 0:\n",
    "            print(f\"{epoch+1:<10} {total_loss.item():<15.6e} {loss_dict['pde']:<15.6e} \"\n",
    "                  f\"{loss_dict['bc_inlet']:<15.6e} {loss_dict['bc_top']:<15.6e} \"\n",
    "                  f\"{loss_dict['bc_bottom']:<15.6e} {loss_dict['ic']:<15.6e}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b9bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss history\n",
    "if len(loss_history) == 0:\n",
    "    print(\"Warning: loss_history is empty. Please run the training cell first.\")\n",
    "else:\n",
    "    loss_history_array = np.array([(h['epoch'], h['total'], h['pde'], h['bc_inlet'], \n",
    "                                    h['bc_top'], h['bc_bottom'], h['ic']) for h in loss_history])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    \n",
    "    # Total loss\n",
    "    axes[0].semilogy(loss_history_array[:, 0], loss_history_array[:, 1], 'b-', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Total Loss', fontsize=12)\n",
    "    axes[0].set_title('Total Loss History', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    if 'n_epochs' in globals():\n",
    "        axes[0].set_xlim([0, n_epochs])\n",
    "    else:\n",
    "        axes[0].set_xlim([0, loss_history_array[-1, 0] + 1])\n",
    "    \n",
    "    # Individual losses\n",
    "    axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 2], 'r-', label='PDE', linewidth=2)\n",
    "    axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 3], 'g-', label='BC Inlet', linewidth=2)\n",
    "    axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 4], 'm-', label='BC Top', linewidth=2)\n",
    "    axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 5], 'c-', label='BC Bottom', linewidth=2)\n",
    "    axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 6], 'y-', label='IC', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title('Individual Loss Components', fontsize=14)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    if 'n_epochs' in globals():\n",
    "        axes[1].set_xlim([0, n_epochs])\n",
    "    else:\n",
    "        axes[1].set_xlim([0, loss_history_array[-1, 0] + 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize solution at final time\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Create grid for visualization\n",
    "    n_x, n_z = 100, 100\n",
    "    x_vis = torch.linspace(x_min, x_max, n_x, device=device).reshape(-1, 1)\n",
    "    z_vis = torch.linspace(z_min, z_max, n_z, device=device).reshape(-1, 1)\n",
    "    X_vis, Z_vis = torch.meshgrid(x_vis.squeeze(), z_vis.squeeze(), indexing='ij')\n",
    "    \n",
    "    # Evaluate at final time\n",
    "    t_final = torch.ones_like(X_vis.reshape(-1, 1)) * t_max\n",
    "    c_final = model(X_vis.reshape(-1, 1), Z_vis.reshape(-1, 1), t_final)\n",
    "    C_final = c_final.reshape(n_x, n_z).cpu().numpy()\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    levels = np.linspace(0.0, 1.0, 51)\n",
    "    im = ax.contourf(X_vis.cpu().numpy(), Z_vis.cpu().numpy(), 1 - C_final, levels=levels, cmap='hot')\n",
    "    ax.set_xlabel(r'$\\tilde{x}$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\tilde{z}$', fontsize=14)\n",
    "    ax.set_title(f'Concentration $c_s$ at $\\\\tilde{{t}}={t_max}$ (Λ={Lambda}, Pe={Pe})', fontsize=14)\n",
    "    cbar = plt.colorbar(im, ax=ax, label='Concentration $c_s$')\n",
    "    ax.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd532353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize solution evolution over time\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    n_x, n_z = 100, 100\n",
    "    x_vis = torch.linspace(x_min, x_max, n_x, device=device).reshape(-1, 1)\n",
    "    z_vis = torch.linspace(z_min, z_max, n_z, device=device).reshape(-1, 1)\n",
    "    X_vis, Z_vis = torch.meshgrid(x_vis.squeeze(), z_vis.squeeze(), indexing='ij')\n",
    "    \n",
    "    # Plot at different times\n",
    "    times = [0.0, 2.5, 5.0, 7.5, 10.0]\n",
    "    levels = np.linspace(0.0, 1.0, 51)\n",
    "    fig, axes = plt.subplots(1, len(times), figsize=(20, 4))\n",
    "    \n",
    "    for idx, t_val in enumerate(times):\n",
    "        t_vis = torch.ones_like(X_vis.reshape(-1, 1)) * t_val\n",
    "        c_vis = model(X_vis.reshape(-1, 1), Z_vis.reshape(-1, 1), t_vis)\n",
    "        C_vis = c_vis.reshape(n_x, n_z).cpu().numpy()\n",
    "        \n",
    "        im = axes[idx].contourf(X_vis.cpu().numpy(), Z_vis.cpu().numpy(), 1-C_vis, levels=levels, cmap='hot')\n",
    "        im.set_clim(0.0, 1.0)  # Explicitly set color limits\n",
    "        axes[idx].set_xlabel(r'$\\tilde{x}$', fontsize=12)\n",
    "        axes[idx].set_ylabel(r'$\\tilde{z}$', fontsize=12)\n",
    "        axes[idx].set_title(f'$\\\\tilde{{t}}={t_val}$', fontsize=12)\n",
    "        axes[idx].set_aspect('equal')\n",
    "        cbar = plt.colorbar(im, ax=axes[idx])\n",
    "        # cbar.set_clim(0.0, 1.0)  # Explicitly set colorbar limits\n",
    "    \n",
    "    plt.suptitle(f'Concentration Evolution (Λ={Lambda}, Pe={Pe})', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a7b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PDE residual at final time\n",
    "model.eval()\n",
    "# Sample points in the domain\n",
    "n_check = 1000\n",
    "x_check = torch.rand(n_check, 1, device=device) * (x_max - x_min) + x_min\n",
    "z_check = torch.rand(n_check, 1, device=device) * (z_max - z_min) + z_min\n",
    "t_check = torch.ones(n_check, 1, device=device) * t_max\n",
    "\n",
    "residual, c_check = compute_pde_residual(x_check, z_check, t_check, model, Lambda, Pe, k)\n",
    "\n",
    "# Detach for statistics (no need for gradients)\n",
    "residual_detached = residual.detach()\n",
    "\n",
    "print(f\"PDE Residual Statistics at t={t_max}:\")\n",
    "print(f\"  Mean: {torch.mean(torch.abs(residual_detached)).item():.6e}\")\n",
    "print(f\"  Std:  {torch.std(residual_detached).item():.6e}\")\n",
    "print(f\"  Max:  {torch.max(torch.abs(residual_detached)).item():.6e}\")\n",
    "print(f\"  Min:  {torch.min(torch.abs(residual_detached)).item():.6e}\")\n",
    "\n",
    "# Plot residual distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.hist(residual_detached.cpu().numpy().flatten(), bins=50, edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('PDE Residual', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Distribution of PDE Residuals at Final Time', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "file_name = './models/pinn_forward.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'Lambda': Lambda,\n",
    "    'Pe': Pe,\n",
    "    'k': k,\n",
    "    'tEnd': tEnd,\n",
    "    'loss_history': loss_history,\n",
    "    'layers': layers,  # Save network architecture\n",
    "    'x_min': x_min, 'x_max': x_max,\n",
    "    'z_min': z_min, 'z_max': z_max,\n",
    "    't_min': t_min, 't_max': t_max,\n",
    "    'pde points': n_pde,\n",
    "    'bc_inlet points': n_bc_inlet,\n",
    "    'bc_top points': n_bc_top,\n",
    "    'bc_bottom points': n_bc_bottom,\n",
    "    'bc_outlet points': n_bc_outlet,\n",
    "    'ic points': n_ic\n",
    "}, file_name)\n",
    "print(f\"Model saved to '{file_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0461ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load a saved model\n",
    "# checkpoint = torch.load('pinn_segregation_model.pth')\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# Lambda = checkpoint['Lambda']\n",
    "# Pe = checkpoint['Pe']\n",
    "# k = checkpoint['k']\n",
    "# tEnd = checkpoint['tEnd']\n",
    "# loss_history = checkpoint['loss_history']\n",
    "# print(\"Model loaded successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
