{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db5272da",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Network (PINN) for Granular Segregation - Phase 2.2\n",
    "\n",
    "This notebook implements a PINN for inverse modeling of the non-dimensionalized segregation PDE.\n",
    "\n",
    "## Phase 2.2: Neural Network for Segregation Velocity\n",
    "\n",
    "In this phase, we replace the functional form of the segregation velocity $\\Lambda(1-\\tilde{x})g(\\tilde{z})(1-c)$ with a **trainable neural network**. The segregation term in the PDE is parameterized as $\\partial/\\partial\\tilde{z}[v_{seg}(\\tilde{x}, \\tilde{z}, \\tilde{t}, \\dot{\\gamma}, c) \\cdot c]$, where $v_{seg}$ is parameterized by a neural network that takes as inputs: spatial coordinates (x̃, z̃), time (t̃), shear rate (γ̇), and concentration (c).\n",
    "\n",
    "## Non-dimensionalized PDE\n",
    "\n",
    "The governing equation is:\n",
    "\n",
    "$$\\frac{\\partial c}{\\partial \\tilde{t}} + \\tilde{u} \\frac{\\partial c}{\\partial \\tilde{x}} + \\tilde{w} \\frac{\\partial c}{\\partial \\tilde{z}} + \\frac{\\partial}{\\partial \\tilde{z}} \\left[ v_{seg}(\\tilde{x}, \\tilde{z}, \\tilde{t}, \\dot{\\gamma}, c) \\cdot c \\right] = \\frac{\\partial}{\\partial \\tilde{z}} \\left[ \\frac{1}{Pe} \\frac{\\partial c}{\\partial \\tilde{z}} \\right]$$\n",
    "\n",
    "Expanding the segregation term:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\tilde{z}} \\left[ v_{seg} \\cdot c \\right] = v_{seg} \\frac{\\partial c}{\\partial \\tilde{z}} + \\frac{\\partial v_{seg}}{\\partial \\tilde{z}} \\cdot c$$\n",
    "\n",
    "Where:\n",
    "- $c$ is the concentration (volume fraction of small particles)\n",
    "- $\\tilde{x} \\in [0, 1]$, $\\tilde{z} \\in [-1, 0]$, $\\tilde{t} \\in [0, t_{end}]$ are dimensionless coordinates\n",
    "- $v_{seg}(\\tilde{x}, \\tilde{z}, \\tilde{t}, \\dot{\\gamma}, c)$ is the **neural network** representing the segregation velocity (replaces $\\Lambda(1-\\tilde{x})g(\\tilde{z})(1-c)$)\n",
    "- $\\dot{\\gamma}$ is the shear rate (computed from $g(\\tilde{z})$)\n",
    "- $Pe$ is the Péclet number\n",
    "- $\\tilde{u}$ and $\\tilde{w}$ are dimensionless velocity profiles\n",
    "\n",
    "## Boundary Conditions\n",
    "\n",
    "- **Inlet** ($\\tilde{x} = 0$): Dirichlet $c = 0.5$ (well-mixed feed)\n",
    "- **Top/Bottom** ($\\tilde{z} = 0, -1$): Neumann $(1/Pe) \\partial c/\\partial \\tilde{z} = v_{seg}(\\tilde{x}, \\tilde{z}, \\tilde{t}, \\dot{\\gamma}, c) \\cdot c$\n",
    "- **Outlet** ($\\tilde{x} = 1$): Natural boundary condition (zero diffusive flux)\n",
    "- **Initial condition**: $c = 0.5$ everywhere at $\\tilde{t} = 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b85ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import grad\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510619b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters from the paper (section 3)\n",
    "Pe = 4.0           # Péclet number\n",
    "D = 1.0 / Pe        # vertical diffusivity\n",
    "k = 2.3             # exponential velocity profile parameter\n",
    "tEnd = 20.0         # final dimensionless time\n",
    "\n",
    "# Domain bounds\n",
    "x_min, x_max = 0.0, 1.0    # x̃ ∈ [0, 1]\n",
    "z_min, z_max = -1.0, 0.0   # z̃ ∈ [-1, 0]\n",
    "t_min, t_max = 0.0, tEnd   # t̃ ∈ [0, tEnd]\n",
    "\n",
    "print(f\"Parameters: Pe={Pe}, D={D:.6f}, k={k}\")\n",
    "print(f\"Domain: x̃ ∈ [{x_min}, {x_max}], z̃ ∈ [{z_min}, {z_max}], t̃ ∈ [{t_min}, {t_max}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b0dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for velocity profiles (u_tilde and w_tilde remain unchanged)\n",
    "def u_tilde(x, z, k):\n",
    "    \"\"\"Dimensionless velocity in x direction\"\"\"\n",
    "    # Convert k to tensor if it's not already a tensor\n",
    "    if not isinstance(k, torch.Tensor):\n",
    "        k = torch.tensor(float(k), dtype=x.dtype, device=x.device)\n",
    "    factor = 0.5 * k * (1 - torch.exp(-k))\n",
    "    return factor * (1 - x) * torch.exp(k * z)\n",
    "\n",
    "def w_tilde(z, k):\n",
    "    \"\"\"Dimensionless velocity in z direction\"\"\"\n",
    "    # Convert k to tensor if it's not already a tensor\n",
    "    if not isinstance(k, torch.Tensor):\n",
    "        k = torch.tensor(float(k), dtype=z.dtype, device=z.device)\n",
    "    factor = 0.5 * (1 - torch.exp(-k))\n",
    "    return factor * (torch.exp(k * z) - 1)\n",
    "\n",
    "# Helper function to compute shear rate gamma_dot\n",
    "def gamma_dot(z, k):\n",
    "    \"\"\"\n",
    "    Compute shear rate gamma_dot from the shear-rate-like profile g(z).\n",
    "    This is used as an input to the segregation velocity network.\n",
    "    \"\"\"\n",
    "    if not isinstance(k, torch.Tensor):\n",
    "        k = torch.tensor(float(k), dtype=z.dtype, device=z.device)\n",
    "    g_val = (k**2 / (2 * (1 - torch.exp(-k)))) * torch.exp(k * z)\n",
    "    return g_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Architecture for Segregation Velocity\n",
    "class SegregationVelocityNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that learns the segregation velocity v_seg(x, z, t, gamma_dot, c).\n",
    "    This replaces the functional form Λ(1-x)g(z)(1-c) from phase 2.1.\n",
    "    Now parameterizes the entire segregation velocity including the (1-c) factor.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers):\n",
    "        super(SegregationVelocityNet, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        for layer in self.layers:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, x, z, t, gamma_dot, c):\n",
    "        \"\"\"\n",
    "        Forward pass: maps (x, z, t, gamma_dot, c) -> v_seg(x, z, t, gamma_dot, c)\n",
    "        Args:\n",
    "            x: x-coordinate tensor [batch_size, 1]\n",
    "            z: z-coordinate tensor [batch_size, 1]\n",
    "            t: t-coordinate tensor [batch_size, 1]\n",
    "            gamma_dot: shear rate tensor [batch_size, 1]\n",
    "            c: concentration tensor [batch_size, 1]\n",
    "        Returns:\n",
    "            v_seg: segregation velocity [batch_size, 1]\n",
    "        \"\"\"\n",
    "        # Concatenate inputs: [x, z, t, gamma_dot, c]\n",
    "        inputs = torch.cat([x, z, t, gamma_dot, c], dim=1)\n",
    "        \n",
    "        # Forward pass through hidden layers\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            inputs = torch.tanh(layer(inputs))\n",
    "        \n",
    "        # Output layer (no activation to allow positive and negative values)\n",
    "        output = self.layers[-1](inputs)\n",
    "        return output\n",
    "\n",
    "# Main PINN Network for Concentration\n",
    "class PINN(nn.Module):\n",
    "    \"\"\"\n",
    "    Physics-Informed Neural Network that predicts concentration c(x, z, t).\n",
    "    This network works together with SegregationVelocityNet to solve the PDE.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers):\n",
    "        super(PINN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        for layer in self.layers:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, x, z, t):\n",
    "        \"\"\"\n",
    "        Forward pass: maps (x, z, t) -> c(x, z, t)\n",
    "        Args:\n",
    "            x: x-coordinate tensor [batch_size, 1]\n",
    "            z: z-coordinate tensor [batch_size, 1]\n",
    "            t: t-coordinate tensor [batch_size, 1]\n",
    "        Returns:\n",
    "            c: concentration [batch_size, 1], bounded between 0 and 1\n",
    "        \"\"\"\n",
    "        # Concatenate inputs: [x, z, t]\n",
    "        inputs = torch.cat([x, z, t], dim=1)\n",
    "        \n",
    "        # Forward pass through hidden layers\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            inputs = torch.tanh(layer(inputs))\n",
    "        \n",
    "        # Output layer with sigmoid activation to bound concentration between 0 and 1\n",
    "        output = torch.sigmoid(self.layers[-1](inputs))\n",
    "        return output\n",
    "\n",
    "# Combined Model that includes both networks\n",
    "class CombinedPINN(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined model that includes both the concentration network and segregation velocity network.\n",
    "    This allows joint training of both networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, conc_layers, seg_layers):\n",
    "        super(CombinedPINN, self).__init__()\n",
    "        self.conc_net = PINN(conc_layers)\n",
    "        self.seg_net = SegregationVelocityNet(seg_layers)\n",
    "    \n",
    "    def forward_conc(self, x, z, t):\n",
    "        \"\"\"Forward pass for concentration prediction\"\"\"\n",
    "        return self.conc_net(x, z, t)\n",
    "    \n",
    "    def forward_seg(self, x, z, t, gamma_dot, c):\n",
    "        \"\"\"Forward pass for segregation velocity prediction\"\"\"\n",
    "        return self.seg_net(x, z, t, gamma_dot, c)\n",
    "\n",
    "# Network architectures\n",
    "conc_layers = [3, 64, 64, 64, 64, 1]  # Concentration network: (x, z, t) -> c\n",
    "seg_layers = [5, 32, 32, 32, 1]        # Segregation velocity network: (x, z, t, gamma_dot, c) -> v_seg\n",
    "\n",
    "model = CombinedPINN(conc_layers, seg_layers).to(device)\n",
    "\n",
    "print(f\"Concentration network architecture: {conc_layers}\")\n",
    "print(f\"Segregation velocity network architecture: {seg_layers}\")\n",
    "print(f\"Total parameters (concentration): {sum(p.numel() for p in model.conc_net.parameters())}\")\n",
    "print(f\"Total parameters (segregation): {sum(p.numel() for p in model.seg_net.parameters())}\")\n",
    "print(f\"Total parameters (combined): {sum(p.numel() for p in model.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19010dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pde_residual(x, z, t, model, Pe, k):\n",
    "    \"\"\"\n",
    "    Compute the PDE residual using neural network for segregation velocity.\n",
    "    \n",
    "    PDE: ∂c/∂t̃ + ũ ∂c/∂x̃ + w̃ ∂c/∂z̃ + ∂/∂z̃ [v_seg(x,z,t,γ_dot,c) * c] = (1/Pe) ∂²c/∂z̃²\n",
    "    \n",
    "    The segregation term is now: ∂/∂z̃ [v_seg*c] = v_seg * ∂c/∂z̃ + (∂v_seg/∂z̃) * c\n",
    "    \n",
    "    Note: v_seg is parameterized as a neural network v_seg(x, z, t, gamma_dot, c),\n",
    "    replacing the functional form Λ(1-x)g(z)(1-c) from phase 2.1.\n",
    "    \"\"\"\n",
    "    # Clone and detach tensors to avoid modifying originals, then enable gradient computation\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    z = z.clone().detach().requires_grad_(True)\n",
    "    t = t.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Forward pass for concentration\n",
    "    c = model.forward_conc(x, z, t)\n",
    "    \n",
    "    # Compute gradients of concentration\n",
    "    c_t = grad(c, t, grad_outputs=torch.ones_like(c), create_graph=True)[0]\n",
    "    c_x = grad(c, x, grad_outputs=torch.ones_like(c), create_graph=True)[0]\n",
    "    c_z = grad(c, z, grad_outputs=torch.ones_like(c), create_graph=True)[0]\n",
    "    \n",
    "    # Second derivative in z\n",
    "    c_zz = grad(c_z, z, grad_outputs=torch.ones_like(c_z), create_graph=True)[0]\n",
    "    \n",
    "    # Velocity profiles (advection)\n",
    "    u_tilde_val = u_tilde(x, z, k)\n",
    "    w_tilde_val = w_tilde(z, k)\n",
    "    \n",
    "    # Compute shear rate gamma_dot\n",
    "    gamma_dot_val = gamma_dot(z, k)\n",
    "    \n",
    "    # Get segregation velocity from neural network: v_seg(x, z, t, gamma_dot, c)\n",
    "    v_seg = model.forward_seg(x, z, t, gamma_dot_val, c)\n",
    "    \n",
    "    # Compute the segregation flux: v_seg * c\n",
    "    seg_flux = v_seg * c\n",
    "    \n",
    "    # Compute derivative of segregation flux with respect to z: ∂/∂z [v_seg*c]\n",
    "    seg_flux_z = grad(seg_flux, z, grad_outputs=torch.ones_like(seg_flux), create_graph=True)[0]\n",
    "    \n",
    "    # Diffusion term\n",
    "    diff_term = (1.0 / Pe) * c_zz\n",
    "    \n",
    "    # Advection term\n",
    "    adv_term = u_tilde_val * c_x + w_tilde_val * c_z\n",
    "    \n",
    "    # Segregation term: ∂/∂z [v_seg*c]\n",
    "    seg_term = seg_flux_z\n",
    "    \n",
    "    # PDE residual: ∂c/∂t - (diffusion - advection - segregation)\n",
    "    residual = c_t - (diff_term - adv_term - seg_term)\n",
    "    \n",
    "    return residual, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8315482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training points\n",
    "def generate_training_data(n_pde, n_bc_inlet, n_bc_top, n_bc_bottom, n_bc_outlet, n_ic):\n",
    "    \"\"\"\n",
    "    Generate collocation points for:\n",
    "    - PDE residual points (interior)\n",
    "    - Boundary condition points\n",
    "    - Initial condition points\n",
    "    \"\"\"\n",
    "    # PDE collocation points (interior domain)\n",
    "    x_pde = torch.rand(n_pde, 1, device=device) * (x_max - x_min) + x_min\n",
    "    z_pde = torch.rand(n_pde, 1, device=device) * (z_max - z_min) + z_min\n",
    "    t_pde = torch.rand(n_pde, 1, device=device) * (t_max - t_min) + t_min\n",
    "    \n",
    "    # Boundary: x̃ = 0 (inlet, Dirichlet: c = 0.5)\n",
    "    x_bc_inlet = torch.zeros(n_bc_inlet, 1, device=device)\n",
    "    z_bc_inlet = torch.rand(n_bc_inlet, 1, device=device) * (z_max - z_min) + z_min\n",
    "    t_bc_inlet = torch.rand(n_bc_inlet, 1, device=device) * (t_max - t_min) + t_min\n",
    "    \n",
    "    # Boundary: z̃ = 0 (top, Neumann)\n",
    "    x_bc_top = torch.rand(n_bc_top, 1, device=device) * (x_max - x_min) + x_min\n",
    "    z_bc_top = torch.zeros(n_bc_top, 1, device=device)\n",
    "    t_bc_top = torch.rand(n_bc_top, 1, device=device) * (t_max - t_min) + t_min\n",
    "    \n",
    "    # Boundary: z̃ = -1 (bottom, Neumann)\n",
    "    x_bc_bottom = torch.rand(n_bc_bottom, 1, device=device) * (x_max - x_min) + x_min\n",
    "    z_bc_bottom = torch.ones(n_bc_bottom, 1, device=device) * z_min\n",
    "    t_bc_bottom = torch.rand(n_bc_bottom, 1, device=device) * (t_max - t_min) + t_min\n",
    "    \n",
    "    # Boundary: x̃ = 1 (outlet, zero diffusive flux - natural BC, handled by PDE)\n",
    "    x_bc_outlet = torch.ones(n_bc_outlet, 1, device=device)\n",
    "    z_bc_outlet = torch.rand(n_bc_outlet, 1, device=device) * (z_max - z_min) + z_min\n",
    "    t_bc_outlet = torch.rand(n_bc_outlet, 1, device=device) * (t_max - t_min) + t_min\n",
    "    \n",
    "    # Initial condition: t̃ = 0, c = 0.5 everywhere\n",
    "    x_ic = torch.rand(n_ic, 1, device=device) * (x_max - x_min) + x_min\n",
    "    z_ic = torch.rand(n_ic, 1, device=device) * (z_max - z_min) + z_min\n",
    "    t_ic = torch.zeros(n_ic, 1, device=device)\n",
    "    \n",
    "    return {\n",
    "        'pde': (x_pde, z_pde, t_pde),\n",
    "        'bc_inlet': (x_bc_inlet, z_bc_inlet, t_bc_inlet),\n",
    "        'bc_top': (x_bc_top, z_bc_top, t_bc_top),\n",
    "        'bc_bottom': (x_bc_bottom, z_bc_bottom, t_bc_bottom),\n",
    "        'bc_outlet': (x_bc_outlet, z_bc_outlet, t_bc_outlet),\n",
    "        'ic': (x_ic, z_ic, t_ic)\n",
    "    }\n",
    "\n",
    "# Number of training points\n",
    "n_pde = 10000\n",
    "n_bc_inlet = 1000\n",
    "n_bc_top = 1000\n",
    "n_bc_bottom = 1000\n",
    "n_bc_outlet = 1000\n",
    "n_ic = 2000\n",
    "\n",
    "train_data = generate_training_data(n_pde, n_bc_inlet, n_bc_top, n_bc_bottom, n_bc_outlet, n_ic)\n",
    "print(\"Training data generated:\")\n",
    "for key, (x, z, t) in train_data.items():\n",
    "    print(f\"  {key}: {x.shape[0]} points\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f79fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, train_data, Pe, k, weights, exp_data=None):\n",
    "    \"\"\"\n",
    "    Compute total loss using neural network for segregation velocity.\n",
    "    L = w_pde * L_pde + w_bc_inlet * L_bc_inlet + w_bc_top * L_bc_top \n",
    "        + w_bc_bottom * L_bc_bottom + w_ic * L_ic + w_data * L_data\n",
    "    \n",
    "    Args:\n",
    "        model: CombinedPINN model (contains both concentration and segregation networks)\n",
    "        train_data: Dictionary of training collocation points\n",
    "        Pe: Péclet number\n",
    "        k: Velocity profile parameter\n",
    "        weights: Dictionary of loss weights\n",
    "        exp_data: Optional experimental data dictionary with keys:\n",
    "            'x', 'z', 't', 'c' (concentration measurements)\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # PDE residual loss\n",
    "    x_pde, z_pde, t_pde = train_data['pde']\n",
    "    residual, _ = compute_pde_residual(x_pde, z_pde, t_pde, model, Pe, k)\n",
    "    loss_pde = torch.mean(residual**2)\n",
    "    total_loss += weights['pde'] * loss_pde\n",
    "    \n",
    "    # Boundary condition: x̃ = 0, c = 0.5 (Dirichlet)\n",
    "    x_in, z_in, t_in = train_data['bc_inlet']\n",
    "    c_in = model.forward_conc(x_in, z_in, t_in)\n",
    "    loss_bc_inlet = torch.mean((c_in - 0.5)**2)\n",
    "    total_loss += weights['bc_inlet'] * loss_bc_inlet\n",
    "    \n",
    "    # Boundary condition: z̃ = 0 (top, Neumann: (1/Pe) ∂c/∂z = v_seg(x,z,t,γ_dot,c) * c)\n",
    "    x_top, z_top, t_top = train_data['bc_top']\n",
    "    x_top = x_top.clone().detach().requires_grad_(True)\n",
    "    z_top = z_top.clone().detach().requires_grad_(True)\n",
    "    t_top = t_top.clone().detach()\n",
    "    c_top = model.forward_conc(x_top, z_top, t_top)\n",
    "    c_z_top = grad(c_top, z_top, grad_outputs=torch.ones_like(c_top), create_graph=True)[0]\n",
    "    gamma_dot_top = gamma_dot(z_top, k)\n",
    "    v_seg_top = model.forward_seg(x_top, z_top, t_top, gamma_dot_top, c_top)\n",
    "    bc_top_target = v_seg_top * c_top  # Segregation flux at boundary\n",
    "    bc_top_pred = (1.0 / Pe) * c_z_top  # Diffusive flux\n",
    "    loss_bc_top = torch.mean((bc_top_pred - bc_top_target)**2)\n",
    "    total_loss += weights['bc_top'] * loss_bc_top\n",
    "    \n",
    "    # Boundary condition: z̃ = -1 (bottom, Neumann: (1/Pe) ∂c/∂z = v_seg(x,z,t,γ_dot,c) * c)\n",
    "    x_bot, z_bot, t_bot = train_data['bc_bottom']\n",
    "    x_bot = x_bot.clone().detach().requires_grad_(True)\n",
    "    z_bot = z_bot.clone().detach().requires_grad_(True)\n",
    "    t_bot = t_bot.clone().detach()\n",
    "    c_bot = model.forward_conc(x_bot, z_bot, t_bot)\n",
    "    c_z_bot = grad(c_bot, z_bot, grad_outputs=torch.ones_like(c_bot), create_graph=True)[0]\n",
    "    gamma_dot_bot = gamma_dot(z_bot, k)\n",
    "    v_seg_bot = model.forward_seg(x_bot, z_bot, t_bot, gamma_dot_bot, c_bot)\n",
    "    bc_bot_target = v_seg_bot * c_bot  # Segregation flux at boundary\n",
    "    bc_bot_pred = (1.0 / Pe) * c_z_bot  # Diffusive flux\n",
    "    loss_bc_bottom = torch.mean((bc_bot_pred - bc_bot_target)**2)\n",
    "    total_loss += weights['bc_bottom'] * loss_bc_bottom\n",
    "    \n",
    "    # Initial condition: t̃ = 0, c = 0.5\n",
    "    x_ic, z_ic, t_ic = train_data['ic']\n",
    "    c_ic = model.forward_conc(x_ic, z_ic, t_ic)\n",
    "    loss_ic = torch.mean((c_ic - 0.5)**2)\n",
    "    total_loss += weights['ic'] * loss_ic\n",
    "    \n",
    "    # Experimental data loss (if provided)\n",
    "    loss_data = torch.tensor(0.0, device=device)\n",
    "    if exp_data is not None:\n",
    "        x_data = exp_data['x']\n",
    "        z_data = exp_data['z']\n",
    "        t_data = exp_data['t']\n",
    "        c_data = exp_data['c']  # Measured concentration values\n",
    "        \n",
    "        # Predict concentration at experimental points\n",
    "        c_pred = model.forward_conc(x_data, z_data, t_data)\n",
    "        loss_data = torch.mean((c_pred - c_data)**2)\n",
    "        total_loss += weights.get('data', 1.0) * loss_data\n",
    "    \n",
    "    # Outlet boundary (x̃ = 1): zero diffusive flux is naturally satisfied\n",
    "    # by the PDE, so we don't need to enforce it explicitly\n",
    "    \n",
    "    loss_dict = {\n",
    "        'pde': loss_pde.item(),\n",
    "        'bc_inlet': loss_bc_inlet.item(),\n",
    "        'bc_top': loss_bc_top.item(),\n",
    "        'bc_bottom': loss_bc_bottom.item(),\n",
    "        'ic': loss_ic.item()\n",
    "    }\n",
    "    \n",
    "    if exp_data is not None:\n",
    "        loss_dict['data'] = loss_data.item()\n",
    "    \n",
    "    return total_loss, loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7123984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Prepare Experimental Data (if available)\n",
    "# ============================================================================\n",
    "# If you have experimental data, load it here and format it as tensors.\n",
    "# The data should contain: x, z, t (coordinates) and c (concentration measurements)\n",
    "\n",
    "use_experimental_data = True  # Set to True when you have experimental data\n",
    "\n",
    "if use_experimental_data:\n",
    "    # Load from CSV file\n",
    "    import pandas as pd\n",
    "    data = pd.read_csv('experimental_data.csv')\n",
    "    x_exp = torch.tensor(data['x'].values, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "    z_exp = torch.tensor(data['z'].values, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "    t_exp = torch.tensor(data['t'].values, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "    c_exp = torch.tensor(data['c'].values, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "    n_exp = x_exp.shape[0]\n",
    "    \n",
    "    exp_data = {\n",
    "        'x': x_exp,\n",
    "        'z': z_exp,\n",
    "        't': t_exp,\n",
    "        'c': c_exp\n",
    "    }\n",
    "    print(f\"Loaded {n_exp} experimental data points\")\n",
    "else:\n",
    "    exp_data = None\n",
    "    print(\"No experimental data provided. Both networks will be learned from physics constraints only.\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Training Setup\n",
    "# ============================================================================\n",
    "# Both networks (concentration and segregation velocity) will be trained jointly\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1000)\n",
    "\n",
    "# Loss weights (can be adjusted)\n",
    "# Increase 'data' weight if you have experimental data to emphasize data fitting\n",
    "loss_weights = {\n",
    "    'pde': 1.0,\n",
    "    'bc_inlet': 10.0,\n",
    "    'bc_top': 10.0,\n",
    "    'bc_bottom': 10.0,\n",
    "    'ic': 10.0,\n",
    "    'data': 50.0 if use_experimental_data else 0.0  # Higher weight for experimental data\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 20000\n",
    "print_interval = 500\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs) # option 2: cosine annealing\n",
    "\n",
    "# Training loop\n",
    "loss_history = []\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"Training both concentration network and segregation velocity network jointly.\")\n",
    "if use_experimental_data:\n",
    "    print(f\"{'Epoch':<10} {'Total Loss':<15} {'PDE':<15} {'BC Inlet':<15} {'BC Top':<15} {'BC Bottom':<15} {'IC':<15} {'Data':<15}\")\n",
    "    print(\"-\" * 120)\n",
    "else:\n",
    "    print(f\"{'Epoch':<10} {'Total Loss':<15} {'PDE':<15} {'BC Inlet':<15} {'BC Top':<15} {'BC Bottom':<15} {'IC':<15}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "try:\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute loss\n",
    "        total_loss, loss_dict = compute_loss(model, train_data, Pe, k, loss_weights, exp_data=exp_data)\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "#         scheduler.step(total_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Store loss history\n",
    "        loss_history.append({\n",
    "            'epoch': epoch,\n",
    "            'total': total_loss.item(),\n",
    "            **loss_dict\n",
    "        })\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % print_interval == 0 or epoch == 0:\n",
    "            if use_experimental_data:\n",
    "                print(f\"{epoch+1:<10} {total_loss.item():<15.6e} {loss_dict['pde']:<15.6e} \"\n",
    "                      f\"{loss_dict['bc_inlet']:<15.6e} {loss_dict['bc_top']:<15.6e} \"\n",
    "                      f\"{loss_dict['bc_bottom']:<15.6e} {loss_dict['ic']:<15.6e} \"\n",
    "                      f\"{loss_dict.get('data', float('nan')):<15.6f}\")\n",
    "            else:\n",
    "                print(f\"{epoch+1:<10} {total_loss.item():<15.6e} {loss_dict['pde']:<15.6e} \"\n",
    "                      f\"{loss_dict['bc_inlet']:<15.6e} {loss_dict['bc_top']:<15.6e} \"\n",
    "                      f\"{loss_dict['bc_bottom']:<15.6e} {loss_dict['ic']:<15.6e}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ce880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss history\n",
    "if len(loss_history) == 0:\n",
    "    print(\"Warning: loss_history is empty. Please run the training cell first.\")\n",
    "else:\n",
    "    if use_experimental_data:\n",
    "        loss_history_array = np.array([(h['epoch'], h['total'], h['pde'], h['bc_inlet'], \n",
    "                                    h['bc_top'], h['bc_bottom'], h['ic'], h['data']) for h in loss_history])\n",
    "    else:\n",
    "        loss_history_array = np.array([(h['epoch'], h['total'], h['pde'], h['bc_inlet'], \n",
    "                                    h['bc_top'], h['bc_bottom'], h['ic']) for h in loss_history])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    \n",
    "    # Total loss\n",
    "    axes[0].semilogy(loss_history_array[:, 0], loss_history_array[:, 1], 'b-', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Total Loss', fontsize=12)\n",
    "    axes[0].set_title('Total Loss History', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    if 'n_epochs' in globals():\n",
    "        axes[0].set_xlim([0, n_epochs])\n",
    "    else:\n",
    "        axes[0].set_xlim([0, loss_history_array[-1, 0] + 1])\n",
    "    \n",
    "    # Individual losses\n",
    "    axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 2], 'r-', label='PDE', linewidth=2)\n",
    "    axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 3], 'g-', label='BC Inlet', linewidth=2)\n",
    "    axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 4], 'm-', label='BC Top', linewidth=2)\n",
    "    axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 5], 'c-', label='BC Bottom', linewidth=2)\n",
    "    axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 6], 'y-', label='IC', linewidth=2)\n",
    "    if use_experimental_data:\n",
    "        axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 7], 'k-', label='Data', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title('Individual Loss Components', fontsize=14)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    if 'n_epochs' in globals():\n",
    "        axes[1].set_xlim([0, n_epochs])\n",
    "    else:\n",
    "        axes[1].set_xlim([0, loss_history_array[-1, 0] + 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned segregation velocity network v_seg(x, z)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Create grid for visualization\n",
    "    n_x, n_z = 100, 100\n",
    "    x_vis = torch.linspace(x_min, x_max, n_x, device=device).reshape(-1, 1)\n",
    "    z_vis = torch.linspace(z_min, z_max, n_z, device=device).reshape(-1, 1)\n",
    "    X_vis, Z_vis = torch.meshgrid(x_vis.squeeze(), z_vis.squeeze(), indexing='ij')\n",
    "    \n",
    "    # Evaluate segregation velocity network at final time\n",
    "    # Need: x, z, t, gamma_dot, c\n",
    "    t_vis = torch.ones_like(X_vis.reshape(-1, 1)) * t_max\n",
    "    gamma_dot_vis = gamma_dot(Z_vis.reshape(-1, 1), k)\n",
    "    c_vis = model.forward_conc(X_vis.reshape(-1, 1), Z_vis.reshape(-1, 1), t_vis)\n",
    "    \n",
    "    # Evaluate segregation velocity network: v_seg(x, z, t, gamma_dot, c)\n",
    "    v_seg_vis = model.forward_seg(X_vis.reshape(-1, 1), Z_vis.reshape(-1, 1), \n",
    "                                   t_vis, gamma_dot_vis, c_vis)\n",
    "    V_seg_vis = v_seg_vis.reshape(n_x, n_z).cpu().numpy()\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    im = ax.contourf(X_vis.cpu().numpy(), Z_vis.cpu().numpy(), V_seg_vis, levels=50, cmap='viridis')\n",
    "    ax.set_xlabel(r'$\\tilde{x}$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\tilde{z}$', fontsize=14)\n",
    "    ax.set_title('Learned Segregation Velocity $v_{seg}(\\\\tilde{x}, \\\\tilde{z}, \\\\tilde{t}, \\\\gamma_dot, c)$ from Neural Network', fontsize=14)\n",
    "    cbar = plt.colorbar(im, ax=ax, label='$v_{seg}$')\n",
    "    ax.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Segregation velocity statistics:\")\n",
    "    print(f\"  Mean: {np.mean(V_seg_vis):.6f}\")\n",
    "    print(f\"  Std:  {np.std(V_seg_vis):.6f}\")\n",
    "    print(f\"  Min:  {np.min(V_seg_vis):.6f}\")\n",
    "    print(f\"  Max:  {np.max(V_seg_vis):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66da7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize solution (concentration) at final time\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Create grid for visualization\n",
    "    n_x, n_z = 100, 100\n",
    "    x_vis = torch.linspace(x_min, x_max, n_x, device=device).reshape(-1, 1)\n",
    "    z_vis = torch.linspace(z_min, z_max, n_z, device=device).reshape(-1, 1)\n",
    "    X_vis, Z_vis = torch.meshgrid(x_vis.squeeze(), z_vis.squeeze(), indexing='ij')\n",
    "    \n",
    "    # Evaluate at final time\n",
    "    t_final = torch.ones_like(X_vis.reshape(-1, 1)) * t_max\n",
    "    c_final = model.forward_conc(X_vis.reshape(-1, 1), Z_vis.reshape(-1, 1), t_final)\n",
    "    C_final = c_final.reshape(n_x, n_z).cpu().numpy()\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    levels = np.linspace(0.0, 1.0, 51)\n",
    "    im = ax.contourf(X_vis.cpu().numpy(), Z_vis.cpu().numpy(), 1 - C_final, levels=levels, cmap='hot')\n",
    "    ax.set_xlabel(r'$\\tilde{x}$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\tilde{z}$', fontsize=14)\n",
    "    ax.set_title(f'Concentration $c_s$ at $\\\\tilde{{t}}={t_max}$ (Pe={Pe})', fontsize=14)\n",
    "    cbar = plt.colorbar(im, ax=ax, label='Concentration $c_s$')\n",
    "    ax.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00bce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize solution evolution over time\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    n_x, n_z = 100, 100\n",
    "    x_vis = torch.linspace(x_min, x_max, n_x, device=device).reshape(-1, 1)\n",
    "    z_vis = torch.linspace(z_min, z_max, n_z, device=device).reshape(-1, 1)\n",
    "    X_vis, Z_vis = torch.meshgrid(x_vis.squeeze(), z_vis.squeeze(), indexing='ij')\n",
    "    \n",
    "    # Plot at different times\n",
    "    times = [0.0, 2.5, 5.0, 7.5, 10.0]\n",
    "    levels = np.linspace(0.0, 1.0, 51)\n",
    "    fig, axes = plt.subplots(1, len(times), figsize=(20, 4))\n",
    "    \n",
    "    for idx, t_val in enumerate(times):\n",
    "        t_vis = torch.ones_like(X_vis.reshape(-1, 1)) * t_val\n",
    "        c_vis = model.forward_conc(X_vis.reshape(-1, 1), Z_vis.reshape(-1, 1), t_vis)\n",
    "        C_vis = c_vis.reshape(n_x, n_z).cpu().numpy()\n",
    "        \n",
    "        im = axes[idx].contourf(X_vis.cpu().numpy(), Z_vis.cpu().numpy(), 1-C_vis, levels=levels, cmap='hot')\n",
    "        im.set_clim(0.0, 1.0)  # Explicitly set color limits\n",
    "        axes[idx].set_xlabel(r'$\\tilde{x}$', fontsize=12)\n",
    "        axes[idx].set_ylabel(r'$\\tilde{z}$', fontsize=12)\n",
    "        axes[idx].set_title(f'$\\\\tilde{{t}}={t_val}$', fontsize=12)\n",
    "        axes[idx].set_aspect('equal')\n",
    "        cbar = plt.colorbar(im, ax=axes[idx])\n",
    "    \n",
    "    plt.suptitle(f'Concentration Evolution (Pe={Pe})', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate movie of solution evolution over time\n",
    "# from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
    "# import matplotlib\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     # Create grid for visualization\n",
    "#     n_x, n_z = 100, 100\n",
    "#     x_vis = torch.linspace(x_min, x_max, n_x, device=device).reshape(-1, 1)\n",
    "#     z_vis = torch.linspace(z_min, z_max, n_z, device=device).reshape(-1, 1)\n",
    "#     X_vis, Z_vis = torch.meshgrid(x_vis.squeeze(), z_vis.squeeze(), indexing='ij')\n",
    "    \n",
    "#     # Time points for animation\n",
    "#     n_frames = 100\n",
    "#     time_points = np.linspace(t_min, t_max, n_frames)\n",
    "    \n",
    "#     # Pre-compute all frames\n",
    "#     print(\"Pre-computing frames for animation...\")\n",
    "#     frames = []\n",
    "#     for i, t_val in enumerate(time_points):\n",
    "#         t_vis = torch.ones_like(X_vis.reshape(-1, 1)) * t_val\n",
    "#         c_vis = model.forward_conc(X_vis.reshape(-1, 1), Z_vis.reshape(-1, 1), t_vis)\n",
    "#         C_vis = c_vis.reshape(n_x, n_z).cpu().numpy()\n",
    "#         frames.append(1 - C_vis)  # Convert to c_s (small particle concentration)\n",
    "#         if (i + 1) % 20 == 0:\n",
    "#             print(f\"  Processed {i + 1}/{n_frames} frames\")\n",
    "    \n",
    "#     # Create figure and axis\n",
    "#     fig, ax = plt.subplots(figsize=(10, 8))\n",
    "#     levels = np.linspace(0.0, 1.0, 51)\n",
    "    \n",
    "#     # Initial frame\n",
    "#     im = ax.contourf(X_vis.cpu().numpy(), Z_vis.cpu().numpy(), frames[0], \n",
    "#                      levels=levels, cmap='hot')\n",
    "#     im.set_clim(0.0, 1.0)\n",
    "#     ax.set_xlabel(r'$\\tilde{x}$', fontsize=14)\n",
    "#     ax.set_ylabel(r'$\\tilde{z}$', fontsize=14)\n",
    "#     ax.set_aspect('equal')\n",
    "#     cbar = plt.colorbar(im, ax=ax, label='Concentration $c_s$')\n",
    "#     title = ax.set_title(f'Concentration Evolution: $\\\\tilde{{t}}={time_points[0]:.2f}$ (Pe={Pe})', \n",
    "#                          fontsize=14)\n",
    "    \n",
    "#     # Animation update function\n",
    "#     def animate(frame_idx):\n",
    "#         # Clear only the contour plot and title, keep axes and colorbar\n",
    "#         for collection in ax.collections:\n",
    "#             collection.remove()\n",
    "#         ax.set_title(f'Concentration Evolution: $\\\\tilde{{t}}={time_points[frame_idx]:.2f}$ (Pe={Pe})', \n",
    "#                      fontsize=14)\n",
    "#         # Create new contour plot\n",
    "#         im = ax.contourf(X_vis.cpu().numpy(), Z_vis.cpu().numpy(), frames[frame_idx], \n",
    "#                          levels=levels, cmap='hot')\n",
    "#         im.set_clim(0.0, 1.0)\n",
    "#         return im.collections\n",
    "    \n",
    "#     # Create animation\n",
    "#     print(\"Creating animation...\")\n",
    "#     anim = FuncAnimation(fig, animate, frames=n_frames, interval=100, blit=False, repeat=True)\n",
    "    \n",
    "#     # Save as movie file\n",
    "#     print(\"Saving movie...\")\n",
    "#     try:\n",
    "#         # Try using FFMpegWriter (requires ffmpeg to be installed)\n",
    "#         writer = FFMpegWriter(fps=10, metadata=dict(artist='PINN'), bitrate=1800)\n",
    "#         anim.save('solution_evolution_phase2.2.mp4', writer=writer)\n",
    "#         print(\"Movie saved as 'solution_evolution_phase2.2.mp4'\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving with FFMpegWriter: {e}\")\n",
    "#         print(\"Trying alternative method with PillowWriter...\")\n",
    "#         try:\n",
    "#             from matplotlib.animation import PillowWriter\n",
    "#             writer = PillowWriter(fps=10)\n",
    "#             anim.save('solution_evolution_phase2.2.gif', writer=writer)\n",
    "#             print(\"Movie saved as 'solution_evolution_phase2.2.gif'\")\n",
    "#         except Exception as e2:\n",
    "#             print(f\"Error saving with PillowWriter: {e2}\")\n",
    "#             print(\"Displaying animation inline instead...\")\n",
    "#             plt.show()\n",
    "    \n",
    "#     # Also display the animation inline\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe7b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PDE residual at final time\n",
    "model.eval()  # Set to eval mode (but we still need gradients for residual computation)\n",
    "# Sample points in the domain\n",
    "n_check = 1000\n",
    "x_check = torch.rand(n_check, 1, device=device) * (x_max - x_min) + x_min\n",
    "z_check = torch.rand(n_check, 1, device=device) * (z_max - z_min) + z_min\n",
    "t_check = torch.ones(n_check, 1, device=device) * t_max\n",
    "\n",
    "# Enable gradients for residual computation\n",
    "x_check.requires_grad_(True)\n",
    "z_check.requires_grad_(True)\n",
    "t_check.requires_grad_(True)\n",
    "\n",
    "residual, c_check = compute_pde_residual(x_check, z_check, t_check, model, Pe, k)\n",
    "\n",
    "# Detach for statistics (no need for gradients)\n",
    "residual_detached = residual.detach()\n",
    "\n",
    "print(f\"PDE Residual Statistics at t={t_max}:\")\n",
    "print(f\"  Mean: {torch.mean(torch.abs(residual_detached)).item():.6e}\")\n",
    "print(f\"  Std:  {torch.std(residual_detached).item():.6e}\")\n",
    "print(f\"  Max:  {torch.max(torch.abs(residual_detached)).item():.6e}\")\n",
    "print(f\"  Min:  {torch.min(torch.abs(residual_detached)).item():.6e}\")\n",
    "\n",
    "# Plot residual distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.hist(residual_detached.cpu().numpy().flatten(), bins=50, edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('PDE Residual', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Distribution of PDE Residuals at Final Time', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ab6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Compare learned segregation velocity with functional form from phase 2.1\n",
    "# This requires loading the model from phase 2.1 or having Lambda and g(z) parameters\n",
    "\n",
    "# For comparison, let's create a functional form based on typical values\n",
    "# You can replace this with actual values from phase 2.1 if available\n",
    "Lambda_comparison = 0.78  # Example value from phase 2.1\n",
    "\n",
    "def g_profile_comparison(z, k):\n",
    "    \"\"\"Shear-rate-like profile g(z̃) for comparison\"\"\"\n",
    "    if not isinstance(k, torch.Tensor):\n",
    "        k = torch.tensor(float(k), dtype=z.dtype, device=z.device)\n",
    "    g_val = (k**2 / (2 * (1 - torch.exp(-k)))) * torch.exp(k * z)\n",
    "    return g_val\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Create grid for comparison\n",
    "    n_x, n_z = 100, 100\n",
    "    x_vis = torch.linspace(x_min, x_max, n_x, device=device).reshape(-1, 1)\n",
    "    z_vis = torch.linspace(z_min, z_max, n_z, device=device).reshape(-1, 1)\n",
    "    X_vis, Z_vis = torch.meshgrid(x_vis.squeeze(), z_vis.squeeze(), indexing='ij')\n",
    "    \n",
    "    # Evaluate at final time for comparison\n",
    "    t_vis = torch.ones_like(X_vis.reshape(-1, 1)) * t_max\n",
    "    gamma_dot_vis = gamma_dot(Z_vis.reshape(-1, 1), k)\n",
    "    c_vis = model.forward_conc(X_vis.reshape(-1, 1), Z_vis.reshape(-1, 1), t_vis)\n",
    "    \n",
    "    # Learned segregation velocity from neural network: v_seg(x, z, t, gamma_dot, c)\n",
    "    v_seg_nn = model.forward_seg(X_vis.reshape(-1, 1), Z_vis.reshape(-1, 1), \n",
    "                                  t_vis, gamma_dot_vis, c_vis)\n",
    "    V_seg_nn = v_seg_nn.reshape(n_x, n_z).cpu().numpy()\n",
    "    \n",
    "    # Functional form from phase 2.1: Lambda * (1 - x) * g(z) * (1 - c)\n",
    "    # Note: In phase 2.2, we parameterize the entire v_seg including (1-c) factor\n",
    "    g_val = g_profile_comparison(Z_vis.reshape(-1, 1), k)\n",
    "    v_seg_func = Lambda_comparison * (1 - X_vis.reshape(-1, 1)) * g_val * (1 - c_vis.reshape(-1, 1))\n",
    "    V_seg_func = v_seg_func.reshape(n_x, n_z).cpu().numpy()\n",
    "    \n",
    "    # Difference\n",
    "    V_seg_diff = V_seg_nn - V_seg_func\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Learned neural network\n",
    "    levels = np.linspace(V_seg_nn.min(), V_seg_nn.max(), 50)\n",
    "    im1 = axes[0].contourf(X_vis.cpu().numpy(), Z_vis.cpu().numpy(), V_seg_nn, cmap='viridis', levels=levels)\n",
    "    axes[0].set_xlabel(r'$\\tilde{x}$', fontsize=12)\n",
    "    axes[0].set_ylabel(r'$\\tilde{z}$', fontsize=12)\n",
    "    axes[0].set_title('Learned Neural Network', fontsize=12)\n",
    "    axes[0].set_aspect('equal')\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # Functional form (from phase 2.1)\n",
    "    im2 = axes[1].contourf(X_vis.cpu().numpy(), Z_vis.cpu().numpy(), V_seg_func, cmap='viridis', levels=levels)\n",
    "    axes[1].set_xlabel(r'$\\tilde{x}$', fontsize=12)\n",
    "    axes[1].set_ylabel(r'$\\tilde{z}$', fontsize=12)\n",
    "    axes[1].set_title(f'Functional Form (Λ={Lambda_comparison:.2f})', fontsize=12)\n",
    "    axes[1].set_aspect('equal')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    # Difference\n",
    "    im3 = axes[2].contourf(X_vis.cpu().numpy(), Z_vis.cpu().numpy(), V_seg_diff, levels=50, cmap='RdBu_r')\n",
    "    axes[2].set_xlabel(r'$\\tilde{x}$', fontsize=12)\n",
    "    axes[2].set_ylabel(r'$\\tilde{z}$', fontsize=12)\n",
    "    axes[2].set_title('Difference (NN - Functional)', fontsize=12)\n",
    "    axes[2].set_aspect('equal')\n",
    "    plt.colorbar(im3, ax=axes[2])\n",
    "    \n",
    "    plt.suptitle('Comparison: Neural Network vs Functional Form for Segregation Velocity', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Difference statistics:\")\n",
    "    print(f\"  Mean: {np.mean(V_seg_diff):.6f}\")\n",
    "    print(f\"  Std:  {np.std(V_seg_diff):.6f}\")\n",
    "    print(f\"  Max:  {np.max(V_seg_diff):.6f}\")\n",
    "    print(f\"  Min:  {np.min(V_seg_diff):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e326f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot residuals between PINN solution and experimental data (if available)\n",
    "if exp_data is None:\n",
    "    print(\"No experimental data available. Cannot compute residuals.\")\n",
    "else:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get experimental data\n",
    "        x_exp = exp_data['x']\n",
    "        z_exp = exp_data['z']\n",
    "        t_exp = exp_data['t']\n",
    "        c_exp = exp_data['c']  # Actual experimental measurements\n",
    "        \n",
    "        # Predict concentrations at experimental points\n",
    "        c_pred = model.forward_conc(x_exp, z_exp, t_exp)\n",
    "        \n",
    "        # Compute residuals (predicted - actual)\n",
    "        residuals = (c_pred - c_exp).cpu().numpy().flatten()\n",
    "        c_pred_np = c_pred.cpu().numpy().flatten()\n",
    "        c_exp_np = c_exp.cpu().numpy().flatten()\n",
    "        \n",
    "        # Statistics\n",
    "        mean_residual = np.mean(residuals)\n",
    "        std_residual = np.std(residuals)\n",
    "        mse = np.mean(residuals**2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = np.mean(np.abs(residuals))\n",
    "        \n",
    "        print(\"Residual Statistics:\")\n",
    "        print(f\"  Mean residual: {mean_residual:.6e}\")\n",
    "        print(f\"  Std residual:  {std_residual:.6e}\")\n",
    "        print(f\"  MSE:           {mse:.6e}\")\n",
    "        print(f\"  RMSE:          {rmse:.6e}\")\n",
    "        print(f\"  MAE:           {mae:.6e}\")\n",
    "        \n",
    "        # Scatter plot: Predicted vs Actual\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.scatter(c_exp_np, c_pred_np, alpha=0.6, s=20, color='steelblue')\n",
    "        # Perfect prediction line (y=x)\n",
    "        min_val = min(c_exp_np.min(), c_pred_np.min())\n",
    "        max_val = max(c_exp_np.max(), c_pred_np.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect prediction')\n",
    "        ax.set_xlabel('Actual Concentration', fontsize=12)\n",
    "        ax.set_ylabel('Predicted Concentration', fontsize=12)\n",
    "        ax.set_title('Predicted vs Actual Concentration', fontsize=14)\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a9d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "file_name = './models/pinn_inverse_NN.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'conc_layers': conc_layers,\n",
    "    'seg_layers': seg_layers,\n",
    "    'Pe': Pe,\n",
    "    'k': k,\n",
    "    'tEnd': tEnd,\n",
    "    'loss_history': loss_history,\n",
    "    'x_min': x_min, 'x_max': x_max,\n",
    "    'z_min': z_min, 'z_max': z_max,\n",
    "    't_min': t_min, 't_max': t_max,\n",
    "    'pde points': n_pde,\n",
    "    'bc_inlet points': n_bc_inlet,\n",
    "    'bc_top points': n_bc_top,\n",
    "    'bc_bottom points': n_bc_bottom,\n",
    "    'bc_outlet points': n_bc_outlet,\n",
    "    'ic points': n_ic,\n",
    "    'exp points': n_exp\n",
    "}, file_name)\n",
    "print(f\"Model saved to '{file_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1e1507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
