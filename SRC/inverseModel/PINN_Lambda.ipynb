{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd6fa25",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Network (PINN) for Granular Segregation - Phase 2.1\n",
    "\n",
    "This notebook implements a PINN for inverse modeling of the non-dimensionalized segregation PDE.\n",
    "\n",
    "## Phase 2.1: Learnable Segregation Parameter Λ\n",
    "\n",
    "In this phase, we learn the segregation parameter $\\Lambda$ as a **learnable parameter** during training. The functional form of the segregation velocity $\\Lambda(1-\\tilde{x})g(\\tilde{z})(1-c)$ is maintained, but instead of using a fixed value for $\\Lambda$, it becomes a trainable parameter that is optimized along with the neural network weights.\n",
    "\n",
    "## Non-dimensionalized PDE\n",
    "\n",
    "The governing equation is:\n",
    "\n",
    "$$\\frac{\\partial c}{\\partial \\tilde{t}} + \\tilde{u} \\frac{\\partial c}{\\partial \\tilde{x}} + \\tilde{w} \\frac{\\partial c}{\\partial \\tilde{z}} + \\Lambda(1 - \\tilde{x}) \\frac{\\partial}{\\partial \\tilde{z}} \\left[ g(\\tilde{z}) c(1-c) \\right] = \\frac{\\partial}{\\partial \\tilde{z}} \\left[ \\frac{1}{Pe} \\frac{\\partial c}{\\partial \\tilde{z}} \\right]$$\n",
    "\n",
    "Expanding the segregation term:\n",
    "\n",
    "$$\\Lambda(1-\\tilde{x}) \\frac{\\partial}{\\partial \\tilde{z}} \\left[ g(\\tilde{z}) c(1-c) \\right] = \\Lambda(1-\\tilde{x}) \\left[ g'(\\tilde{z}) c(1-c) + g(\\tilde{z})(1-2c) \\frac{\\partial c}{\\partial \\tilde{z}} \\right]$$\n",
    "\n",
    "Where:\n",
    "- $c$ is the concentration (volume fraction of small particles)\n",
    "- $\\tilde{x} \\in [0, 1]$, $\\tilde{z} \\in [-1, 0]$, $\\tilde{t} \\in [0, t_{end}]$ are dimensionless coordinates\n",
    "- $\\Lambda$ is the **learnable segregation parameter** (optimized during training)\n",
    "- $Pe$ is the Péclet number\n",
    "- $\\tilde{u}$ and $\\tilde{w}$ are dimensionless velocity profiles\n",
    "- $g(\\tilde{z})$ is a shear-rate-like profile\n",
    "\n",
    "## Boundary Conditions\n",
    "\n",
    "- **Inlet** ($\\tilde{x} = 0$): Dirichlet $c = 0.5$ (well-mixed feed)\n",
    "- **Top/Bottom** ($\\tilde{z} = 0, -1$): Neumann $(1/Pe) \\partial c/\\partial \\tilde{z} = \\Lambda(1-\\tilde{x}) g(\\tilde{z}) c(1-c)$\n",
    "- **Outlet** ($\\tilde{x} = 1$): Natural boundary condition (zero diffusive flux)\n",
    "- **Initial condition**: $c = 0.5$ everywhere at $\\tilde{t} = 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa83b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import grad\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe57e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters from the paper (section 3)\n",
    "# Lambda will be a learnable parameter (defined in the PINN class)\n",
    "Lambda_init = 0.3949  # Initial guess for Λ (will be learned)\n",
    "Pe = 4.0           # Péclet number\n",
    "D = 1.0 / Pe        # vertical diffusivity\n",
    "k = 2.3             # exponential velocity profile parameter\n",
    "tEnd = 20.0         # final dimensionless time\n",
    "\n",
    "# Domain bounds\n",
    "x_min, x_max = 0.0, 1.0    # x̃ ∈ [0, 1]\n",
    "z_min, z_max = -1.0, 0.0   # z̃ ∈ [-1, 0]\n",
    "t_min, t_max = 0.0, tEnd   # t̃ ∈ [0, tEnd]\n",
    "\n",
    "print(f\"Parameters: Λ_init={Lambda_init}, Pe={Pe}, D={D:.6f}, k={k}\")\n",
    "print(f\"Domain: x̃ ∈ [{x_min}, {x_max}], z̃ ∈ [{z_min}, {z_max}], t̃ ∈ [{t_min}, {t_max}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5032ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for velocity profiles and g(z̃)\n",
    "def u_tilde(x, z, k):\n",
    "    \"\"\"Dimensionless velocity in x direction\"\"\"\n",
    "    # Convert k to tensor if it's not already a tensor\n",
    "    if not isinstance(k, torch.Tensor):\n",
    "        k = torch.tensor(float(k), dtype=x.dtype, device=x.device)\n",
    "    factor = 0.5 * k * (1 - torch.exp(-k))\n",
    "    return factor * (1 - x) * torch.exp(k * z)\n",
    "\n",
    "def w_tilde(z, k):\n",
    "    \"\"\"Dimensionless velocity in z direction\"\"\"\n",
    "    # Convert k to tensor if it's not already a tensor\n",
    "    if not isinstance(k, torch.Tensor):\n",
    "        k = torch.tensor(float(k), dtype=z.dtype, device=z.device)\n",
    "    factor = 0.5 * (1 - torch.exp(-k))\n",
    "    return factor * (torch.exp(k * z) - 1)\n",
    "\n",
    "def g_profile(z, k):\n",
    "    \"\"\"Shear-rate-like profile g(z̃) and its derivative\"\"\"\n",
    "    # Convert k to tensor if it's not already a tensor\n",
    "    if not isinstance(k, torch.Tensor):\n",
    "        k = torch.tensor(float(k), dtype=z.dtype, device=z.device)\n",
    "    g_val = (k**2 / (2 * (1 - torch.exp(-k)))) * torch.exp(k * z)\n",
    "    g_prime = k * g_val\n",
    "    return g_val, g_prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bfad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Architecture with Learnable Lambda\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers, lambda_init=0.78):\n",
    "        super(PINN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        for layer in self.layers:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        # Lambda as a learnable parameter\n",
    "        # Use log-space to ensure Lambda stays positive during training\n",
    "        self.log_lambda = nn.Parameter(torch.tensor(np.log(lambda_init), dtype=torch.float32))\n",
    "    \n",
    "    @property\n",
    "    def Lambda(self):\n",
    "        \"\"\"Get Lambda value (always positive due to exp)\"\"\"\n",
    "        return torch.exp(self.log_lambda)\n",
    "    \n",
    "    def forward(self, x, z, t):\n",
    "        # Concatenate inputs: [x, z, t]\n",
    "        inputs = torch.cat([x, z, t], dim=1)\n",
    "        \n",
    "        # Forward pass through hidden layers\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            inputs = torch.tanh(layer(inputs))\n",
    "        \n",
    "        # Output layer (no activation for regression)\n",
    "        # output = self.layers[-1](inputs)\n",
    "        \n",
    "        # Output layer with sigmoid activation to bound concentration between 0 and 1\n",
    "        output = torch.sigmoid(self.layers[-1](inputs))\n",
    "        return output\n",
    "\n",
    "# Network architecture: 3 inputs (x, z, t) -> hidden layers -> 1 output (c)\n",
    "layers = [3, 64, 64, 64, 64, 1]\n",
    "model = PINN(layers, lambda_init=Lambda_init).to(device)\n",
    "print(f\"Model architecture: {layers}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Initial Lambda: {model.Lambda.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9a8978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pde_residual(x, z, t, model, Pe, k):\n",
    "    \"\"\"\n",
    "    Compute the PDE residual:\n",
    "    ∂c/∂t̃ = (1/Pe) ∂²c/∂z̃² - ũ ∂c/∂x̃ - w̃ ∂c/∂z̃ \n",
    "            - Λ(1-x̃)[ g'(z̃)c(1-c) + g(z̃)(1-2c)∂c/∂z̃ ]\n",
    "    Note: Lambda is now a learnable parameter accessed via model.Lambda\n",
    "    \"\"\"\n",
    "    # Clone and detach tensors to avoid modifying originals, then enable gradient computation\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    z = z.clone().detach().requires_grad_(True)\n",
    "    t = t.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Forward pass\n",
    "    c = model(x, z, t)\n",
    "    \n",
    "    # Compute gradients\n",
    "    c_t = grad(c, t, grad_outputs=torch.ones_like(c), create_graph=True)[0]\n",
    "    c_x = grad(c, x, grad_outputs=torch.ones_like(c), create_graph=True)[0]\n",
    "    c_z = grad(c, z, grad_outputs=torch.ones_like(c), create_graph=True)[0]\n",
    "    \n",
    "    # Second derivative in z\n",
    "    c_zz = grad(c_z, z, grad_outputs=torch.ones_like(c_z), create_graph=True)[0]\n",
    "    \n",
    "    # Velocity profiles\n",
    "    u_tilde_val = u_tilde(x, z, k)\n",
    "    w_tilde_val = w_tilde(z, k)\n",
    "    \n",
    "    # g profile and its derivative\n",
    "    g_val, g_prime = g_profile(z, k)\n",
    "    \n",
    "    # Get learnable Lambda from model\n",
    "    Lambda = model.Lambda\n",
    "    \n",
    "    # Diffusion term\n",
    "    diff_term = (1.0 / Pe) * c_zz\n",
    "    \n",
    "    # Advection term\n",
    "    adv_term = u_tilde_val * c_x + w_tilde_val * c_z\n",
    "    \n",
    "    # Segregation term (using learnable Lambda)\n",
    "    seg_term = Lambda * (1 - x) * (g_prime * c * (1 - c) + g_val * (1 - 2*c) * c_z)\n",
    "    \n",
    "    # PDE residual: ∂c/∂t - (diffusion - advection - segregation)\n",
    "    residual = c_t - (diff_term - adv_term - seg_term)\n",
    "    \n",
    "    return residual, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd61db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training points\n",
    "def generate_training_data(n_pde, n_bc_inlet, n_bc_top, n_bc_bottom, n_bc_outlet, n_ic):\n",
    "    \"\"\"\n",
    "    Generate collocation points for:\n",
    "    - PDE residual points (interior)\n",
    "    - Boundary condition points\n",
    "    - Initial condition points\n",
    "    \"\"\"\n",
    "    # PDE collocation points (interior domain)\n",
    "    x_pde = torch.rand(n_pde, 1, device=device) * (x_max - x_min) + x_min\n",
    "    z_pde = torch.rand(n_pde, 1, device=device) * (z_max - z_min) + z_min\n",
    "    t_pde = torch.rand(n_pde, 1, device=device) * (t_max - t_min) + t_min\n",
    "    \n",
    "    # Boundary: x̃ = 0 (inlet, Dirichlet: c = 0.5)\n",
    "    x_bc_inlet = torch.zeros(n_bc_inlet, 1, device=device)\n",
    "    z_bc_inlet = torch.rand(n_bc_inlet, 1, device=device) * (z_max - z_min) + z_min\n",
    "    t_bc_inlet = torch.rand(n_bc_inlet, 1, device=device) * (t_max - t_min) + t_min\n",
    "    \n",
    "    # Boundary: z̃ = 0 (top, Neumann)\n",
    "    x_bc_top = torch.rand(n_bc_top, 1, device=device) * (x_max - x_min) + x_min\n",
    "    z_bc_top = torch.zeros(n_bc_top, 1, device=device)\n",
    "    t_bc_top = torch.rand(n_bc_top, 1, device=device) * (t_max - t_min) + t_min\n",
    "    \n",
    "    # Boundary: z̃ = -1 (bottom, Neumann)\n",
    "    x_bc_bottom = torch.rand(n_bc_bottom, 1, device=device) * (x_max - x_min) + x_min\n",
    "    z_bc_bottom = torch.ones(n_bc_bottom, 1, device=device) * z_min\n",
    "    t_bc_bottom = torch.rand(n_bc_bottom, 1, device=device) * (t_max - t_min) + t_min\n",
    "    \n",
    "    # Boundary: x̃ = 1 (outlet, zero diffusive flux - natural BC, handled by PDE)\n",
    "    x_bc_outlet = torch.ones(n_bc_outlet, 1, device=device)\n",
    "    z_bc_outlet = torch.rand(n_bc_outlet, 1, device=device) * (z_max - z_min) + z_min\n",
    "    t_bc_outlet = torch.rand(n_bc_outlet, 1, device=device) * (t_max - t_min) + t_min\n",
    "    \n",
    "    # Initial condition: t̃ = 0, c = 0.5 everywhere\n",
    "    x_ic = torch.rand(n_ic, 1, device=device) * (x_max - x_min) + x_min\n",
    "    z_ic = torch.rand(n_ic, 1, device=device) * (z_max - z_min) + z_min\n",
    "    t_ic = torch.zeros(n_ic, 1, device=device)\n",
    "    \n",
    "    return {\n",
    "        'pde': (x_pde, z_pde, t_pde),\n",
    "        'bc_inlet': (x_bc_inlet, z_bc_inlet, t_bc_inlet),\n",
    "        'bc_top': (x_bc_top, z_bc_top, t_bc_top),\n",
    "        'bc_bottom': (x_bc_bottom, z_bc_bottom, t_bc_bottom),\n",
    "        'bc_outlet': (x_bc_outlet, z_bc_outlet, t_bc_outlet),\n",
    "        'ic': (x_ic, z_ic, t_ic)\n",
    "    }\n",
    "\n",
    "# Number of training points\n",
    "n_pde = 10000\n",
    "n_bc_inlet = 1000\n",
    "n_bc_top = 1000\n",
    "n_bc_bottom = 1000\n",
    "n_bc_outlet = 1000\n",
    "n_ic = 2000\n",
    "\n",
    "train_data = generate_training_data(n_pde, n_bc_inlet, n_bc_top, n_bc_bottom, n_bc_outlet, n_ic)\n",
    "print(\"Training data generated:\")\n",
    "for key, (x, z, t) in train_data.items():\n",
    "    print(f\"  {key}: {x.shape[0]} points\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad98d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, train_data, Pe, k, weights, exp_data=None):\n",
    "    \"\"\"\n",
    "    Compute total loss:\n",
    "    L = w_pde * L_pde + w_bc_inlet * L_bc_inlet + w_bc_top * L_bc_top \n",
    "        + w_bc_bottom * L_bc_bottom + w_ic * L_ic + w_data * L_data\n",
    "    \n",
    "    Args:\n",
    "        model: PINN model (contains learnable Lambda)\n",
    "        train_data: Dictionary of training collocation points\n",
    "        Pe: Péclet number\n",
    "        k: Velocity profile parameter\n",
    "        weights: Dictionary of loss weights\n",
    "        exp_data: Optional experimental data dictionary with keys:\n",
    "            'x', 'z', 't', 'c' (concentration measurements)\n",
    "    \"\"\"\n",
    "    # Get learnable Lambda from model\n",
    "    Lambda = model.Lambda\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # PDE residual loss\n",
    "    x_pde, z_pde, t_pde = train_data['pde']\n",
    "    residual, _ = compute_pde_residual(x_pde, z_pde, t_pde, model, Pe, k)\n",
    "    loss_pde = torch.mean(residual**2)\n",
    "    total_loss += weights['pde'] * loss_pde\n",
    "    \n",
    "    # Boundary condition: x̃ = 0, c = 0.5 (Dirichlet)\n",
    "    x_in, z_in, t_in = train_data['bc_inlet']\n",
    "    c_in = model(x_in, z_in, t_in)\n",
    "    loss_bc_inlet = torch.mean((c_in - 0.5)**2)\n",
    "    total_loss += weights['bc_inlet'] * loss_bc_inlet\n",
    "    \n",
    "    # Boundary condition: z̃ = 0 (top, Neumann: (1/Pe) ∂c/∂z = Λ(1-x) g(z) c(1-c))\n",
    "    # Clone tensors to avoid modifying originals\n",
    "    x_top, z_top, t_top = train_data['bc_top']\n",
    "    x_top = x_top.clone().detach().requires_grad_(True)\n",
    "    z_top = z_top.clone().detach().requires_grad_(True)\n",
    "    t_top = t_top.clone().detach()\n",
    "    c_top = model(x_top, z_top, t_top)\n",
    "    c_z_top = grad(c_top, z_top, grad_outputs=torch.ones_like(c_top), create_graph=True)[0]\n",
    "    g_val_top, _ = g_profile(z_top, k)\n",
    "    bc_top_target = Lambda * (1 - x_top) * g_val_top * c_top * (1 - c_top)\n",
    "    bc_top_pred = (1.0 / Pe) * c_z_top\n",
    "    loss_bc_top = torch.mean((bc_top_pred - bc_top_target)**2)\n",
    "    total_loss += weights['bc_top'] * loss_bc_top\n",
    "    \n",
    "    # Boundary condition: z̃ = -1 (bottom, Neumann: (1/Pe) ∂c/∂z = Λ(1-x) g(z) c(1-c))\n",
    "    # Clone tensors to avoid modifying originals\n",
    "    x_bot, z_bot, t_bot = train_data['bc_bottom']\n",
    "    x_bot = x_bot.clone().detach().requires_grad_(True)\n",
    "    z_bot = z_bot.clone().detach().requires_grad_(True)\n",
    "    t_bot = t_bot.clone().detach()\n",
    "    c_bot = model(x_bot, z_bot, t_bot)\n",
    "    c_z_bot = grad(c_bot, z_bot, grad_outputs=torch.ones_like(c_bot), create_graph=True)[0]\n",
    "    g_val_bot, _ = g_profile(z_bot, k)\n",
    "    bc_bot_target = Lambda * (1 - x_bot) * g_val_bot * c_bot * (1 - c_bot)\n",
    "    bc_bot_pred = (1.0 / Pe) * c_z_bot\n",
    "    loss_bc_bottom = torch.mean((bc_bot_pred - bc_bot_target)**2)\n",
    "    total_loss += weights['bc_bottom'] * loss_bc_bottom\n",
    "    \n",
    "    # Initial condition: t̃ = 0, c = 0.5\n",
    "    x_ic, z_ic, t_ic = train_data['ic']\n",
    "    c_ic = model(x_ic, z_ic, t_ic)\n",
    "    loss_ic = torch.mean((c_ic - 0.5)**2)\n",
    "    total_loss += weights['ic'] * loss_ic\n",
    "    \n",
    "    # Experimental data loss (if provided)\n",
    "    loss_data = torch.tensor(0.0, device=device)\n",
    "    if exp_data is not None:\n",
    "        x_data = exp_data['x']\n",
    "        z_data = exp_data['z']\n",
    "        t_data = exp_data['t']\n",
    "        c_data = exp_data['c']  # Measured concentration values\n",
    "        \n",
    "        # Predict concentration at experimental points\n",
    "        c_pred = model(x_data, z_data, t_data)\n",
    "        loss_data = torch.mean((c_pred - c_data)**2)\n",
    "        total_loss += weights.get('data', 1.0) * loss_data\n",
    "    \n",
    "    # Outlet boundary (x̃ = 1): zero diffusive flux is naturally satisfied\n",
    "    # by the PDE, so we don't need to enforce it explicitly\n",
    "    \n",
    "    loss_dict = {\n",
    "        'pde': loss_pde.item(),\n",
    "        'bc_inlet': loss_bc_inlet.item(),\n",
    "        'bc_top': loss_bc_top.item(),\n",
    "        'bc_bottom': loss_bc_bottom.item(),\n",
    "        'ic': loss_ic.item(),\n",
    "        'lambda': Lambda.item()\n",
    "    }\n",
    "    \n",
    "    if exp_data is not None:\n",
    "        loss_dict['data'] = loss_data.item()\n",
    "    \n",
    "    return total_loss, loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28971d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Prepare Experimental Data (if available)\n",
    "# ============================================================================\n",
    "# If you have experimental data, load it here and format it as tensors.\n",
    "# The data should contain: x, z, t (coordinates) and c (concentration measurements)\n",
    "\n",
    "# Example: Load experimental data from a file or create synthetic data\n",
    "# Replace this with your actual data loading code\n",
    "use_experimental_data = True  # Set to True when you have experimental data\n",
    "\n",
    "if use_experimental_data:\n",
    "    # Option 1: Load from CSV file\n",
    "    import pandas as pd\n",
    "    data = pd.read_csv('experimental_data.csv')\n",
    "    x_exp = torch.tensor(data['x'].values, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "    z_exp = torch.tensor(data['z'].values, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "    t_exp = torch.tensor(data['t'].values, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "    c_exp = torch.tensor(data['c'].values, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "    n_exp = x_exp.shape[0]\n",
    "    # # Option 2: Create synthetic data for testing (replace with real data)\n",
    "    # n_exp = 100  # Number of experimental data points\n",
    "    # x_exp = torch.rand(n_exp, 1, device=device) * (x_max - x_min) + x_min\n",
    "    # z_exp = torch.rand(n_exp, 1, device=device) * (z_max - z_min) + z_min\n",
    "    # t_exp = torch.rand(n_exp, 1, device=device) * (t_max - t_min) + t_min\n",
    "    # # For testing, generate synthetic measurements (replace with actual measurements)\n",
    "    # c_exp = torch.rand(n_exp, 1, device=device) * 0.3 + 0.35  # Random values between 0.35 and 0.65\n",
    "    \n",
    "    exp_data = {\n",
    "        'x': x_exp,\n",
    "        'z': z_exp,\n",
    "        't': t_exp,\n",
    "        'c': c_exp\n",
    "    }\n",
    "    print(f\"Loaded {n_exp} experimental data points\")\n",
    "else:\n",
    "    exp_data = None\n",
    "    print(\"No experimental data provided. Lambda will be learned from physics constraints only.\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Training Setup\n",
    "# ============================================================================\n",
    "# Note: model.parameters() now includes the learnable Lambda parameter\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1000)\n",
    "\n",
    "# Loss weights (can be adjusted)\n",
    "# Increase 'data' weight if you have experimental data to emphasize data fitting\n",
    "loss_weights = {\n",
    "    'pde': 1.0,\n",
    "    'bc_inlet': 10.0,\n",
    "    'bc_top': 10.0,\n",
    "    'bc_bottom': 10.0,\n",
    "    'ic': 10.0,\n",
    "    'data': 50.0 if use_experimental_data else 0.0  # Higher weight for experimental data\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 20000\n",
    "print_interval = 500\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs) # option 2: cosine annealing\n",
    "\n",
    "# Training loop\n",
    "loss_history = []\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "if use_experimental_data:\n",
    "    print(f\"{'Epoch':<10} {'Total Loss':<15} {'PDE':<15} {'BC Inlet':<15} {'BC Top':<15} {'BC Bottom':<15} {'IC':<15} {'Lambda':<15} {'Data':<15}\")\n",
    "    print(\"-\" * 120)\n",
    "else:\n",
    "    print(f\"{'Epoch':<10} {'Total Loss':<15} {'PDE':<15} {'BC Inlet':<15} {'BC Top':<15} {'BC Bottom':<15} {'IC':<15} {'Lambda':<15}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "try:\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute loss\n",
    "        total_loss, loss_dict = compute_loss(model, train_data, Pe, k, loss_weights, exp_data=exp_data)\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "#         scheduler.step(total_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Store loss history\n",
    "        loss_history.append({\n",
    "            'epoch': epoch,\n",
    "            'total': total_loss.item(),\n",
    "            **loss_dict\n",
    "        })\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % print_interval == 0 or epoch == 0:\n",
    "            if use_experimental_data:\n",
    "                print(f\"{epoch+1:<10} {total_loss.item():<15.6e} {loss_dict['pde']:<15.6e} \"\n",
    "                      f\"{loss_dict['bc_inlet']:<15.6e} {loss_dict['bc_top']:<15.6e} \"\n",
    "                      f\"{loss_dict['bc_bottom']:<15.6e} {loss_dict['ic']:<15.6e} \"\n",
    "                      f\"{loss_dict.get('lambda', float('nan')):<15.6f} {loss_dict.get('data', float('nan')):<15.6f}\")\n",
    "            else:\n",
    "                print(f\"{epoch+1:<10} {total_loss.item():<15.6e} {loss_dict['pde']:<15.6e} \"\n",
    "                      f\"{loss_dict['bc_inlet']:<15.6e} {loss_dict['bc_top']:<15.6e} \"\n",
    "                      f\"{loss_dict['bc_bottom']:<15.6e} {loss_dict['ic']:<15.6e} \"\n",
    "                      f\"{loss_dict.get('lambda', float('nan')):<15.6f}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b9bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss history\n",
    "if len(loss_history) == 0:\n",
    "    print(\"Warning: loss_history is empty. Please run the training cell first.\")\n",
    "else:\n",
    "    if use_experimental_data:\n",
    "        loss_history_array = np.array([(h['epoch'], h['total'], h['pde'], h['bc_inlet'], \n",
    "                                    h['bc_top'], h['bc_bottom'], h['ic'], h['data']) for h in loss_history])\n",
    "    else:\n",
    "        loss_history_array = np.array([(h['epoch'], h['total'], h['pde'], h['bc_inlet'], \n",
    "                                    h['bc_top'], h['bc_bottom'], h['ic']) for h in loss_history])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    \n",
    "    # Total loss\n",
    "    axes[0].semilogy(loss_history_array[:, 0], loss_history_array[:, 1], 'b-', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Total Loss', fontsize=12)\n",
    "    axes[0].set_title('Total Loss History', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    if 'n_epochs' in globals():\n",
    "        axes[0].set_xlim([0, n_epochs])\n",
    "    else:\n",
    "        axes[0].set_xlim([0, loss_history_array[-1, 0] + 1])\n",
    "    \n",
    "    # Individual losses\n",
    "    axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 2], 'r-', label='PDE', linewidth=2)\n",
    "    axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 3], 'g-', label='BC Inlet', linewidth=2)\n",
    "    axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 4], 'm-', label='BC Top', linewidth=2)\n",
    "    axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 5], 'c-', label='BC Bottom', linewidth=2)\n",
    "    axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 6], 'y-', label='IC', linewidth=2)\n",
    "    if use_experimental_data:\n",
    "        axes[1].semilogy(loss_history_array[:, 0], loss_history_array[:, 7], 'k-', label='Data', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title('Individual Loss Components', fontsize=14)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    if 'n_epochs' in globals():\n",
    "        axes[1].set_xlim([0, n_epochs])\n",
    "    else:\n",
    "        axes[1].set_xlim([0, loss_history_array[-1, 0] + 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize solution at final time\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Create grid for visualization\n",
    "    n_x, n_z = 100, 100\n",
    "    x_vis = torch.linspace(x_min, x_max, n_x, device=device).reshape(-1, 1)\n",
    "    z_vis = torch.linspace(z_min, z_max, n_z, device=device).reshape(-1, 1)\n",
    "    X_vis, Z_vis = torch.meshgrid(x_vis.squeeze(), z_vis.squeeze(), indexing='ij')\n",
    "    \n",
    "    # Evaluate at final time\n",
    "    t_final = torch.ones_like(X_vis.reshape(-1, 1)) * t_max\n",
    "    c_final = model(X_vis.reshape(-1, 1), Z_vis.reshape(-1, 1), t_final)\n",
    "    C_final = c_final.reshape(n_x, n_z).cpu().numpy()\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    levels = np.linspace(0.0, 1.0, 51)\n",
    "    im = ax.contourf(X_vis.cpu().numpy(), Z_vis.cpu().numpy(), 1 - C_final, levels=levels, cmap='hot')\n",
    "    ax.set_xlabel(r'$\\tilde{x}$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\tilde{z}$', fontsize=14)\n",
    "    ax.set_title(f'Concentration $c_s$ at $\\\\tilde{{t}}={t_max}$ (Λ={model.Lambda.item():.2f}, Pe={Pe})', fontsize=14)\n",
    "    cbar = plt.colorbar(im, ax=ax, label='Concentration $c_s$')\n",
    "    ax.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd532353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize solution evolution over time\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    n_x, n_z = 100, 100\n",
    "    x_vis = torch.linspace(x_min, x_max, n_x, device=device).reshape(-1, 1)\n",
    "    z_vis = torch.linspace(z_min, z_max, n_z, device=device).reshape(-1, 1)\n",
    "    X_vis, Z_vis = torch.meshgrid(x_vis.squeeze(), z_vis.squeeze(), indexing='ij')\n",
    "    \n",
    "    # Plot at different times\n",
    "    times = [0.0, 2.5, 5.0, 7.5, 10.0]\n",
    "    levels = np.linspace(0.0, 1.0, 51)\n",
    "    fig, axes = plt.subplots(1, len(times), figsize=(20, 4))\n",
    "    \n",
    "    for idx, t_val in enumerate(times):\n",
    "        t_vis = torch.ones_like(X_vis.reshape(-1, 1)) * t_val\n",
    "        c_vis = model(X_vis.reshape(-1, 1), Z_vis.reshape(-1, 1), t_vis)\n",
    "        C_vis = c_vis.reshape(n_x, n_z).cpu().numpy()\n",
    "        \n",
    "        im = axes[idx].contourf(X_vis.cpu().numpy(), Z_vis.cpu().numpy(), 1-C_vis, levels=levels, cmap='hot')\n",
    "        im.set_clim(0.0, 1.0)  # Explicitly set color limits\n",
    "        axes[idx].set_xlabel(r'$\\tilde{x}$', fontsize=12)\n",
    "        axes[idx].set_ylabel(r'$\\tilde{z}$', fontsize=12)\n",
    "        axes[idx].set_title(f'$\\\\tilde{{t}}={t_val}$', fontsize=12)\n",
    "        axes[idx].set_aspect('equal')\n",
    "        cbar = plt.colorbar(im, ax=axes[idx])\n",
    "        # cbar.set_clim(0.0, 1.0)  # Explicitly set colorbar limits\n",
    "    \n",
    "    plt.suptitle(f'Concentration Evolution (Λ={model.Lambda.item():.2f}, Pe={Pe})', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate movie of solution evolution over time\n",
    "from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
    "import matplotlib\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Create grid for visualization\n",
    "    n_x, n_z = 100, 100\n",
    "    x_vis = torch.linspace(x_min, x_max, n_x, device=device).reshape(-1, 1)\n",
    "    z_vis = torch.linspace(z_min, z_max, n_z, device=device).reshape(-1, 1)\n",
    "    X_vis, Z_vis = torch.meshgrid(x_vis.squeeze(), z_vis.squeeze(), indexing='ij')\n",
    "    \n",
    "    # Time points for animation\n",
    "    n_frames = 100\n",
    "    time_points = np.linspace(t_min, t_max, n_frames)\n",
    "    \n",
    "    # Pre-compute all frames\n",
    "    print(\"Pre-computing frames for animation...\")\n",
    "    frames = []\n",
    "    for i, t_val in enumerate(time_points):\n",
    "        t_vis = torch.ones_like(X_vis.reshape(-1, 1)) * t_val\n",
    "        c_vis = model(X_vis.reshape(-1, 1), Z_vis.reshape(-1, 1), t_vis)\n",
    "        C_vis = c_vis.reshape(n_x, n_z).cpu().numpy()\n",
    "        frames.append(1 - C_vis)  # Convert to c_s (small particle concentration)\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"  Processed {i + 1}/{n_frames} frames\")\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    levels = np.linspace(0.0, 1.0, 51)\n",
    "    \n",
    "    # Initial frame\n",
    "    im = ax.contourf(X_vis.cpu().numpy(), Z_vis.cpu().numpy(), frames[0], \n",
    "                     levels=levels, cmap='hot')\n",
    "    im.set_clim(0.0, 1.0)\n",
    "    ax.set_xlabel(r'$\\tilde{x}$', fontsize=14)\n",
    "    ax.set_ylabel(r'$\\tilde{z}$', fontsize=14)\n",
    "    ax.set_aspect('equal')\n",
    "    cbar = plt.colorbar(im, ax=ax, label='Concentration $c_s$')\n",
    "    title = ax.set_title(f'Concentration Evolution: $\\\\tilde{{t}}={time_points[0]:.2f}$ (Λ={model.Lambda.item():.2f}, Pe={Pe})', \n",
    "                         fontsize=14)\n",
    "    \n",
    "    # Animation update function\n",
    "    def animate(frame_idx):\n",
    "        # Clear only the contour plot and title, keep axes and colorbar\n",
    "        for collection in ax.collections:\n",
    "            collection.remove()\n",
    "        ax.set_title(f'Concentration Evolution: $\\\\tilde{{t}}={time_points[frame_idx]:.2f}$ (Λ={model.Lambda.item():.2f}, Pe={Pe})', \n",
    "                     fontsize=14)\n",
    "        # Create new contour plot\n",
    "        im = ax.contourf(X_vis.cpu().numpy(), Z_vis.cpu().numpy(), frames[frame_idx], \n",
    "                         levels=levels, cmap='hot')\n",
    "        im.set_clim(0.0, 1.0)\n",
    "        return im.collections\n",
    "    \n",
    "    # Create animation\n",
    "    print(\"Creating animation...\")\n",
    "    anim = FuncAnimation(fig, animate, frames=n_frames, interval=100, blit=False, repeat=True)\n",
    "    \n",
    "    # Save as movie file\n",
    "    print(\"Saving movie...\")\n",
    "    try:\n",
    "        # Try using FFMpegWriter (requires ffmpeg to be installed)\n",
    "        writer = FFMpegWriter(fps=10, metadata=dict(artist='PINN'), bitrate=1800)\n",
    "        anim.save('solution_evolution_phase2.1.mp4', writer=writer)\n",
    "        print(\"Movie saved as 'solution_evolution_phase2.1.mp4'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving with FFMpegWriter: {e}\")\n",
    "        print(\"Trying alternative method with PillowWriter...\")\n",
    "        try:\n",
    "            from matplotlib.animation import PillowWriter\n",
    "            writer = PillowWriter(fps=10)\n",
    "            anim.save('solution_evolution_phase2.1.gif', writer=writer)\n",
    "            print(\"Movie saved as 'solution_evolution_phase2.1.gif'\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Error saving with PillowWriter: {e2}\")\n",
    "            print(\"Displaying animation inline instead...\")\n",
    "            plt.show()\n",
    "    \n",
    "    # Also display the animation inline\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a7b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PDE residual at final time\n",
    "model.eval()\n",
    "# Sample points in the domain\n",
    "n_check = 1000\n",
    "x_check = torch.rand(n_check, 1, device=device) * (x_max - x_min) + x_min\n",
    "z_check = torch.rand(n_check, 1, device=device) * (z_max - z_min) + z_min\n",
    "t_check = torch.ones(n_check, 1, device=device) * t_max\n",
    "\n",
    "residual, c_check = compute_pde_residual(x_check, z_check, t_check, model, Pe, k)\n",
    "\n",
    "# Detach for statistics (no need for gradients)\n",
    "residual_detached = residual.detach()\n",
    "\n",
    "print(f\"PDE Residual Statistics at t={t_max}:\")\n",
    "print(f\"  Mean: {torch.mean(torch.abs(residual_detached)).item():.6e}\")\n",
    "print(f\"  Std:  {torch.std(residual_detached).item():.6e}\")\n",
    "print(f\"  Max:  {torch.max(torch.abs(residual_detached)).item():.6e}\")\n",
    "print(f\"  Min:  {torch.min(torch.abs(residual_detached)).item():.6e}\")\n",
    "\n",
    "# Plot residual distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.hist(residual_detached.cpu().numpy().flatten(), bins=50, edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('PDE Residual', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Distribution of PDE Residuals at Final Time', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6133edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Lambda evolution during training\n",
    "if len(loss_history) == 0:\n",
    "    print(\"Warning: loss_history is empty. Please run the training cell first.\")\n",
    "else:\n",
    "    lambda_values = [h['lambda'] for h in loss_history]\n",
    "    epochs = [h['epoch'] for h in loss_history]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(epochs, lambda_values, 'b-', linewidth=2)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Lambda', fontsize=12)\n",
    "    ax.set_title('Lambda Evolution During Training', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if 'n_epochs' in globals():\n",
    "        ax.set_xlim([0, n_epochs])\n",
    "    else:\n",
    "        ax.set_xlim([0, epochs[-1] + 1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0facbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot residuals between PINN solution and experimental data\n",
    "if exp_data is None:\n",
    "    print(\"No experimental data available. Cannot compute residuals.\")\n",
    "else:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get experimental data\n",
    "        x_exp = exp_data['x']\n",
    "        z_exp = exp_data['z']\n",
    "        t_exp = exp_data['t']\n",
    "        c_exp = exp_data['c']  # Actual experimental measurements\n",
    "        \n",
    "        # Predict concentrations at experimental points\n",
    "        c_pred = model(x_exp, z_exp, t_exp)\n",
    "        \n",
    "        # Compute residuals (predicted - actual)\n",
    "        residuals = (c_pred - c_exp).cpu().numpy().flatten()\n",
    "        c_pred_np = c_pred.cpu().numpy().flatten()\n",
    "        c_exp_np = c_exp.cpu().numpy().flatten()\n",
    "        \n",
    "        # Statistics\n",
    "        mean_residual = np.mean(residuals)\n",
    "        std_residual = np.std(residuals)\n",
    "        mse = np.mean(residuals**2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = np.mean(np.abs(residuals))\n",
    "        \n",
    "        print(\"Residual Statistics:\")\n",
    "        print(f\"  Mean residual: {mean_residual:.6e}\")\n",
    "        print(f\"  Std residual:  {std_residual:.6e}\")\n",
    "        print(f\"  MSE:           {mse:.6e}\")\n",
    "        print(f\"  RMSE:          {rmse:.6e}\")\n",
    "        print(f\"  MAE:           {mae:.6e}\")\n",
    "        \n",
    "        # Create comprehensive residual plots\n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # 1. Histogram of residuals\n",
    "        ax1 = plt.subplot(2, 3, 1)\n",
    "        ax1.hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "        ax1.axvline(0, color='r', linestyle='--', linewidth=2, label='Zero residual')\n",
    "        ax1.axvline(mean_residual, color='g', linestyle='--', linewidth=2, label=f'Mean: {mean_residual:.4e}')\n",
    "        ax1.set_xlabel('Residual (Predicted - Actual)', fontsize=12)\n",
    "        ax1.set_ylabel('Frequency', fontsize=12)\n",
    "        ax1.set_title('Distribution of Residuals', fontsize=14)\n",
    "        ax1.legend(fontsize=10)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Scatter plot: Predicted vs Actual\n",
    "        ax2 = plt.subplot(2, 3, 2)\n",
    "        ax2.scatter(c_exp_np, c_pred_np, alpha=0.6, s=20, color='steelblue')\n",
    "        # Perfect prediction line (y=x)\n",
    "        min_val = min(c_exp_np.min(), c_pred_np.min())\n",
    "        max_val = max(c_exp_np.max(), c_pred_np.max())\n",
    "        ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect prediction')\n",
    "        ax2.set_xlabel('Actual Concentration', fontsize=12)\n",
    "        ax2.set_ylabel('Predicted Concentration', fontsize=12)\n",
    "        ax2.set_title('Predicted vs Actual Concentration', fontsize=14)\n",
    "        ax2.legend(fontsize=10)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_aspect('equal', adjustable='box')\n",
    "        \n",
    "        # 3. Residual vs Predicted (to check for heteroscedasticity)\n",
    "        ax3 = plt.subplot(2, 3, 3)\n",
    "        ax3.scatter(c_pred_np, residuals, alpha=0.6, s=20, color='steelblue')\n",
    "        ax3.axhline(0, color='r', linestyle='--', linewidth=2)\n",
    "        ax3.set_xlabel('Predicted Concentration', fontsize=12)\n",
    "        ax3.set_ylabel('Residual', fontsize=12)\n",
    "        ax3.set_title('Residual vs Predicted', fontsize=14)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Residual vs x coordinate\n",
    "        ax4 = plt.subplot(2, 3, 4)\n",
    "        x_exp_np = x_exp.cpu().numpy().flatten()\n",
    "        ax4.scatter(x_exp_np, residuals, alpha=0.6, s=20, color='steelblue')\n",
    "        ax4.axhline(0, color='r', linestyle='--', linewidth=2)\n",
    "        ax4.set_xlabel(r'$\\tilde{x}$', fontsize=12)\n",
    "        ax4.set_ylabel('Residual', fontsize=12)\n",
    "        ax4.set_title('Residual vs x-coordinate', fontsize=14)\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Residual vs z coordinate\n",
    "        ax5 = plt.subplot(2, 3, 5)\n",
    "        z_exp_np = z_exp.cpu().numpy().flatten()\n",
    "        ax5.scatter(z_exp_np, residuals, alpha=0.6, s=20, color='steelblue')\n",
    "        ax5.axhline(0, color='r', linestyle='--', linewidth=2)\n",
    "        ax5.set_xlabel(r'$\\tilde{z}$', fontsize=12)\n",
    "        ax5.set_ylabel('Residual', fontsize=12)\n",
    "        ax5.set_title('Residual vs z-coordinate', fontsize=14)\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Residual vs time\n",
    "        ax6 = plt.subplot(2, 3, 6)\n",
    "        t_exp_np = t_exp.cpu().numpy().flatten()\n",
    "        ax6.scatter(t_exp_np, residuals, alpha=0.6, s=20, color='steelblue')\n",
    "        ax6.axhline(0, color='r', linestyle='--', linewidth=2)\n",
    "        ax6.set_xlabel(r'$\\tilde{t}$', fontsize=12)\n",
    "        ax6.set_ylabel('Residual', fontsize=12)\n",
    "        ax6.set_title('Residual vs Time', fontsize=14)\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Additional: Q-Q plot for residual normality check\n",
    "        from scipy import stats\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        stats.probplot(residuals, dist=\"norm\", plot=ax)\n",
    "        ax.set_title('Q-Q Plot of Residuals (Normality Check)', fontsize=14)\n",
    "        ax.set_xlabel('Theoretical Quantiles', fontsize=12)\n",
    "        ax.set_ylabel('Sample Quantiles', fontsize=12)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "file_name = './models/pinn_inverse_Lambda.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'Lambda': model.Lambda.item(),  # Save the learned Lambda value\n",
    "    'Lambda_init': Lambda_init,     # Save initial Lambda for reference\n",
    "    'Pe': Pe,\n",
    "    'k': k,\n",
    "    'tEnd': tEnd,\n",
    "    'loss_history': loss_history,\n",
    "    'layers': layers,  # Save network architecture\n",
    "    'x_min': x_min, 'x_max': x_max,\n",
    "    'z_min': z_min, 'z_max': z_max,\n",
    "    't_min': t_min, 't_max': t_max,\n",
    "    'pde points': n_pde,\n",
    "    'bc_inlet points': n_bc_inlet,\n",
    "    'bc_top points': n_bc_top,\n",
    "    'bc_bottom points': n_bc_bottom,\n",
    "    'bc_outlet points': n_bc_outlet,\n",
    "    'ic points': n_ic,\n",
    "    'exp points': n_exp\n",
    "}, file_name)\n",
    "print(f\"Model saved to '{file_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0461ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load a saved model\n",
    "# checkpoint = torch.load('pinn_segregation_model.pth')\n",
    "# \n",
    "# # Restore model architecture and state\n",
    "# if 'layers' in checkpoint:\n",
    "#     layers = checkpoint['layers']\n",
    "#     model = PINN(layers, lambda_init=checkpoint.get('Lambda_init', checkpoint['Lambda'])).to(device)\n",
    "# else:\n",
    "#     # If layers not saved, use default architecture\n",
    "#     layers = [3, 50, 50, 50, 50, 1]\n",
    "#     model = PINN(layers, lambda_init=checkpoint.get('Lambda_init', checkpoint['Lambda'])).to(device)\n",
    "# \n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# \n",
    "# # Restore parameters\n",
    "# Lambda_init = checkpoint.get('Lambda_init', checkpoint['Lambda'])  # Use Lambda_init if available, else use Lambda\n",
    "# Pe = checkpoint['Pe']\n",
    "# k = checkpoint['k']\n",
    "# tEnd = checkpoint['tEnd']\n",
    "# loss_history = checkpoint['loss_history']\n",
    "# \n",
    "# # Update domain bounds based on tEnd\n",
    "# t_max = tEnd\n",
    "# \n",
    "# print(\"Model loaded successfully!\")\n",
    "# print(f\"  Learned Lambda: {model.Lambda.item():.6f}\")\n",
    "# print(f\"  Initial Lambda: {Lambda_init:.6f}\")\n",
    "# print(f\"  Pe: {Pe}, k: {k}, tEnd: {tEnd}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
